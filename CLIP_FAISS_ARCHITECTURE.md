# ğŸš€ CLIP + FAISS åœ–åƒç›¸ä¼¼æ€§æœå°‹å¼•æ“æ¶æ§‹

**å»ºç«‹æ—¥æœŸ**: 2025-10-04
**ç›®æ¨™**: æ•´åˆ CLIP å’Œ FAISS æ‰“é€ è¶…å¼·åœ–åƒç›¸ä¼¼æ€§æœå°‹ç³»çµ±

---

## ğŸ“‹ ç›®éŒ„

- [ç³»çµ±æ¦‚è¿°](#ç³»çµ±æ¦‚è¿°)
- [æŠ€è¡“æ¶æ§‹](#æŠ€è¡“æ¶æ§‹)
- [CLIP vs ResNet50 å°æ¯”](#clip-vs-resnet50-å°æ¯”)
- [å¯¦ä½œè¨ˆåŠƒ](#å¯¦ä½œè¨ˆåŠƒ)
- [API è¨­è¨ˆ](#api-è¨­è¨ˆ)
- [ä½¿ç”¨å ´æ™¯](#ä½¿ç”¨å ´æ™¯)

---

## ğŸ¯ ç³»çµ±æ¦‚è¿°

### ç•¶å‰æ¶æ§‹
```
STL åœ–ç‰‡ â†’ ResNet50 ç‰¹å¾µæå– â†’ FAISS ç´¢å¼• â†’ ç›¸ä¼¼åº¦æœå°‹
```

### å‡ç´šæ¶æ§‹
```
STL åœ–ç‰‡ â†’ CLIP å¤šæ¨¡æ…‹ç‰¹å¾µæå– â†’ FAISS ç´¢å¼• â†’ æ™ºèƒ½ç›¸ä¼¼åº¦æœå°‹
              â†“
         æ–‡å­—æè¿° â†’ CLIP æ–‡å­—ç·¨ç¢¼ â†’ è·¨æ¨¡æ…‹æœå°‹
```

### æ ¸å¿ƒå„ªå‹¢

#### CLIP çš„å„ªå‹¢
1. **å¤šæ¨¡æ…‹ç†è§£**: åŒæ™‚ç†è§£åœ–åƒå’Œæ–‡å­—
2. **èªç¾©è±å¯Œ**: æ¯” ResNet50 æ›´æ·±å±¤çš„èªç¾©ç†è§£
3. **é›¶æ¨£æœ¬å­¸ç¿’**: ç„¡éœ€å¤§é‡è¨“ç·´å³å¯è­˜åˆ¥æ–°é¡åˆ¥
4. **æ–‡å­—æœåœ–**: æ”¯æ´ç”¨è‡ªç„¶èªè¨€æè¿°æœå°‹åœ–ç‰‡
5. **åœ–æœåœ–å¢å¼·**: æ›´ç²¾æº–çš„è¦–è¦ºç›¸ä¼¼æ€§

#### FAISS çš„å„ªå‹¢
1. **è¶…å¿«é€Ÿæœå°‹**: æ¯«ç§’ç´šéŸ¿æ‡‰
2. **å¤§è¦æ¨¡æ”¯æ´**: æ”¯æ´ç™¾è¬ç´šç‰¹å¾µå‘é‡
3. **GPU åŠ é€Ÿ**: å……åˆ†åˆ©ç”¨ç¡¬é«”è³‡æº
4. **å¤šç¨®ç´¢å¼•**: ç²¾ç¢ºæœå°‹ / è¿‘ä¼¼æœå°‹

---

## ğŸ—ï¸ æŠ€è¡“æ¶æ§‹

### 1. ç‰¹å¾µæå–å±¤

#### CLIP æ¨¡å‹é¸æ“‡
```python
# æ¨è–¦ä½¿ç”¨çš„ CLIP æ¨¡å‹
- ViT-B/32: å¿«é€Ÿï¼Œé©åˆå³æ™‚æ‡‰ç”¨
- ViT-L/14: é«˜ç²¾åº¦ï¼Œé©åˆé›¢ç·šå»ºç´¢å¼•
- RN50x64: è¶…é«˜ç²¾åº¦ï¼Œéœ€è¦æ›´å¤šè³‡æº
```

#### ç‰¹å¾µç¶­åº¦
```python
# CLIP ç‰¹å¾µç¶­åº¦
ViT-B/32:  512 ç¶­
ViT-L/14:  768 ç¶­
RN50x64:   1024 ç¶­

# ResNet50 (ç¾æœ‰)
ResNet50:  2048 ç¶­
```

### 2. FAISS ç´¢å¼•çµæ§‹

```
faiss_indices/
â”œâ”€â”€ clip_vit_b32.index       # CLIP ViT-B/32 ç´¢å¼•
â”œâ”€â”€ clip_vit_l14.index        # CLIP ViT-L/14 ç´¢å¼• (å¯é¸)
â”œâ”€â”€ resnet50.index            # ResNet50 ç´¢å¼• (ä¿ç•™å‘å¾Œå…¼å®¹)
â”œâ”€â”€ clip_labels.pkl           # æ¨™ç±¤æ˜ å°„
â””â”€â”€ metadata.json             # ç´¢å¼•å…ƒæ•¸æ“š
```

### 3. æ··åˆæœå°‹ç­–ç•¥

```python
# ç­–ç•¥ 1: ç´”åœ–åƒæœå°‹ (CLIP)
Query Image â†’ CLIP Encoder â†’ FAISS Search â†’ Top-K Results

# ç­–ç•¥ 2: æ–‡å­—æœåœ– (è·¨æ¨¡æ…‹)
Query Text â†’ CLIP Text Encoder â†’ FAISS Search â†’ Top-K Results

# ç­–ç•¥ 3: æ··åˆæœå°‹ (åœ– + æ–‡)
Query Image + Text â†’ CLIP Fusion â†’ FAISS Search â†’ Top-K Results

# ç­–ç•¥ 4: å¤šæ¨¡å‹èåˆ (æœ€å¼·)
Query â†’ [CLIP + ResNet50] â†’ Score Fusion â†’ Top-K Results
```

---

## ğŸ“Š CLIP vs ResNet50 å°æ¯”

| ç‰¹æ€§ | ResNet50 (ç¾æœ‰) | CLIP | å„ªå‹¢ |
|------|----------------|------|------|
| **ç‰¹å¾µç¶­åº¦** | 2048 | 512-1024 | CLIP æ›´ç·Šæ¹Š |
| **èªç¾©ç†è§£** | è¦–è¦ºç‰¹å¾µ | å¤šæ¨¡æ…‹èªç¾© | âœ… CLIP |
| **æ–‡å­—æœåœ–** | âŒ ä¸æ”¯æ´ | âœ… æ”¯æ´ | âœ… CLIP |
| **é›¶æ¨£æœ¬å­¸ç¿’** | âŒ éœ€è¨“ç·´ | âœ… åŸç”Ÿæ”¯æ´ | âœ… CLIP |
| **æœå°‹ç²¾åº¦** | é«˜ | æ¥µé«˜ | âœ… CLIP |
| **æ¨ç†é€Ÿåº¦** | å¿« | ä¸­ç­‰ | âœ… ResNet50 |
| **è¨˜æ†¶é«”éœ€æ±‚** | ä¸­ | ä¸­-é«˜ | - ç›¸ç•¶ |
| **è¨“ç·´éœ€æ±‚** | éœ€è¦ | é è¨“ç·´å³ç”¨ | âœ… CLIP |

### å»ºè­°ç­–ç•¥

**é›™å¼•æ“æ¨¡å¼**ï¼š
- ä¿ç•™ ResNet50 ç”¨æ–¼å¿«é€Ÿæª¢ç´¢
- CLIP ç”¨æ–¼ç²¾ç¢ºèªç¾©æœå°‹
- æä¾›é¸é …è®“ç”¨æˆ¶é¸æ“‡å¼•æ“

---

## ğŸš€ å¯¦ä½œè¨ˆåŠƒ

### Phase 1: CLIP æ•´åˆ (åŸºç¤)

#### 1.1 å®‰è£ä¾è³´
```bash
pip install openai-clip torch torchvision
# æˆ–ä½¿ç”¨ transformers
pip install transformers
```

#### 1.2 å‰µå»º CLIP ç‰¹å¾µæå–å™¨
```python
# clip_feature_extractor.py
import torch
import clip
from PIL import Image

class CLIPFeatureExtractor:
    def __init__(self, model_name="ViT-B/32", device="cuda"):
        self.device = device
        self.model, self.preprocess = clip.load(model_name, device=device)

    def extract_image_features(self, image_path):
        """æå–åœ–åƒç‰¹å¾µ"""
        image = Image.open(image_path)
        image_input = self.preprocess(image).unsqueeze(0).to(self.device)

        with torch.no_grad():
            features = self.model.encode_image(image_input)
            features = features / features.norm(dim=-1, keepdim=True)

        return features.cpu().numpy()[0]

    def extract_text_features(self, text):
        """æå–æ–‡å­—ç‰¹å¾µ"""
        text_input = clip.tokenize([text]).to(self.device)

        with torch.no_grad():
            features = self.model.encode_text(text_input)
            features = features / features.norm(dim=-1, keepdim=True)

        return features.cpu().numpy()[0]
```

#### 1.3 å»ºç«‹ CLIP-FAISS ç´¢å¼•
```python
# clip_faiss_builder.py
import faiss
import numpy as np

class CLIPFAISSIndexBuilder:
    def __init__(self, dimension=512):
        self.dimension = dimension
        # ä½¿ç”¨å…§ç©ç´¢å¼• (cosine similarity)
        self.index = faiss.IndexFlatIP(dimension)

    def build_from_dataset(self, dataset_dir):
        """å¾è³‡æ–™é›†å»ºç«‹ç´¢å¼•"""
        extractor = CLIPFeatureExtractor()

        features_list = []
        labels_list = []

        for class_name in os.listdir(dataset_dir):
            class_dir = os.path.join(dataset_dir, class_name)
            for img_file in os.listdir(class_dir):
                img_path = os.path.join(class_dir, img_file)

                # æå–ç‰¹å¾µ
                features = extractor.extract_image_features(img_path)
                features_list.append(features)
                labels_list.append(class_name)

        # å»ºç«‹ç´¢å¼•
        features_array = np.array(features_list).astype('float32')
        self.index.add(features_array)

        # ä¿å­˜
        faiss.write_index(self.index, "clip_faiss.index")
        with open("clip_labels.pkl", "wb") as f:
            pickle.dump(labels_list, f)
```

### Phase 2: æ™ºèƒ½æœå°‹ API

#### 2.1 åœ–åƒç›¸ä¼¼æ€§æœå°‹
```python
@app.route('/api/search/image', methods=['POST'])
def search_by_image():
    """ä»¥åœ–æœåœ– API"""
    file = request.files['image']
    k = int(request.form.get('k', 5))

    # æå–ç‰¹å¾µ
    features = clip_extractor.extract_image_features(file)

    # FAISS æœå°‹
    distances, indices = clip_index.search(features.reshape(1, -1), k)

    results = []
    for i, idx in enumerate(indices[0]):
        results.append({
            'class_name': labels[idx],
            'similarity': float(distances[0][i]),
            'rank': i + 1
        })

    return jsonify({'success': True, 'results': results})
```

#### 2.2 æ–‡å­—æœåœ– API
```python
@app.route('/api/search/text', methods=['POST'])
def search_by_text():
    """æ–‡å­—æœåœ– API"""
    query_text = request.json.get('query')
    k = int(request.json.get('k', 5))

    # æå–æ–‡å­—ç‰¹å¾µ
    features = clip_extractor.extract_text_features(query_text)

    # FAISS æœå°‹
    distances, indices = clip_index.search(features.reshape(1, -1), k)

    results = []
    for i, idx in enumerate(indices[0]):
        results.append({
            'class_name': labels[idx],
            'similarity': float(distances[0][i]),
            'description': query_text,
            'rank': i + 1
        })

    return jsonify({'success': True, 'results': results})
```

#### 2.3 æ··åˆæœå°‹ API
```python
@app.route('/api/search/hybrid', methods=['POST'])
def hybrid_search():
    """æ··åˆæœå°‹ï¼šåœ–åƒ + æ–‡å­—"""
    file = request.files.get('image')
    text = request.form.get('text', '')
    k = int(request.form.get('k', 5))

    # æå–ä¸¦èåˆç‰¹å¾µ
    img_features = clip_extractor.extract_image_features(file)
    text_features = clip_extractor.extract_text_features(text)

    # åŠ æ¬Šèåˆ (å¯èª¿æ•´æ¬Šé‡)
    alpha = 0.6  # åœ–åƒæ¬Šé‡
    beta = 0.4   # æ–‡å­—æ¬Šé‡
    combined_features = alpha * img_features + beta * text_features
    combined_features = combined_features / np.linalg.norm(combined_features)

    # FAISS æœå°‹
    distances, indices = clip_index.search(combined_features.reshape(1, -1), k)

    return jsonify({'success': True, 'results': build_results(distances, indices)})
```

### Phase 3: å‰ç«¯æœå°‹ä»‹é¢

#### 3.1 æœå°‹é é¢çµæ§‹
```html
<!-- templates/search/index.html -->
<div class="search-container">
    <!-- æœå°‹æ¨¡å¼é¸æ“‡ -->
    <ul class="nav nav-tabs">
        <li><a data-tab="image">ä»¥åœ–æœåœ–</a></li>
        <li><a data-tab="text">æ–‡å­—æœåœ–</a></li>
        <li><a data-tab="hybrid">æ··åˆæœå°‹</a></li>
    </ul>

    <!-- ä»¥åœ–æœåœ– -->
    <div id="image-search">
        <div class="upload-area">æ‹–æ‹‰åœ–ç‰‡æˆ–é»æ“Šä¸Šå‚³</div>
        <button>æœå°‹ç›¸ä¼¼åœ–ç‰‡</button>
    </div>

    <!-- æ–‡å­—æœåœ– -->
    <div id="text-search">
        <input type="text" placeholder="æè¿°ä½ è¦æ‰¾çš„ STL æ¨¡å‹...">
        <button>æœå°‹</button>
    </div>

    <!-- æ··åˆæœå°‹ -->
    <div id="hybrid-search">
        <div class="upload-area">ä¸Šå‚³åƒè€ƒåœ–ç‰‡</div>
        <input type="text" placeholder="è£œå……æ–‡å­—æè¿°...">
        <button>æ™ºèƒ½æœå°‹</button>
    </div>

    <!-- æœå°‹çµæœ -->
    <div class="search-results">
        <!-- å‹•æ…‹è¼‰å…¥çµæœ -->
    </div>
</div>
```

#### 3.2 çµæœå±•ç¤º
```html
<!-- æœå°‹çµæœå¡ç‰‡ -->
<div class="result-card">
    <img src="/dataset/model_name/img_001.png">
    <div class="result-info">
        <h5>æ¨¡å‹åç¨±</h5>
        <div class="similarity-bar">
            <div class="progress" style="width: 95%">95% ç›¸ä¼¼</div>
        </div>
        <span class="badge">Rank #1</span>
    </div>
</div>
```

---

## ğŸ¨ é€²éšåŠŸèƒ½

### 1. å¤šå¼•æ“èåˆæœå°‹
```python
def multi_engine_search(query_image, k=10):
    """èåˆ CLIP å’Œ ResNet50 çš„æœå°‹çµæœ"""

    # CLIP æœå°‹
    clip_results = clip_engine.search(query_image, k=k)

    # ResNet50 æœå°‹
    resnet_results = resnet_engine.search(query_image, k=k)

    # åˆ†æ•¸èåˆ (Reciprocal Rank Fusion)
    fused_scores = {}
    for i, result in enumerate(clip_results):
        fused_scores[result['class']] = 1.0 / (i + 1)

    for i, result in enumerate(resnet_results):
        if result['class'] in fused_scores:
            fused_scores[result['class']] += 0.5 / (i + 1)
        else:
            fused_scores[result['class']] = 0.5 / (i + 1)

    # æ’åºä¸¦è¿”å›
    sorted_results = sorted(fused_scores.items(),
                          key=lambda x: x[1], reverse=True)

    return sorted_results[:k]
```

### 2. æ™ºèƒ½æŸ¥è©¢æ“´å±•
```python
# è‡ªå‹•æ“´å±•æœå°‹æŸ¥è©¢
query_templates = [
    "a 3D model of {object}",
    "{object} from front view",
    "{object} rendered in high quality",
    "CAD model of {object}"
]

def expand_text_query(base_query):
    """æ“´å±•æ–‡å­—æŸ¥è©¢ä»¥æé«˜å¬å›ç‡"""
    expanded_queries = [template.format(object=base_query)
                       for template in query_templates]

    # å°æ¯å€‹æ“´å±•æŸ¥è©¢é€²è¡Œæœå°‹ä¸¦èåˆçµæœ
    all_results = []
    for query in expanded_queries:
        results = clip_engine.search_by_text(query)
        all_results.extend(results)

    # å»é‡ä¸¦æ’åº
    return deduplicate_and_rank(all_results)
```

### 3. è¦–è¦ºè§£é‡‹
```python
# ä½¿ç”¨ Grad-CAM è§£é‡‹ç‚ºä»€éº¼å…©å€‹åœ–ç‰‡ç›¸ä¼¼
def explain_similarity(query_img, result_img):
    """ç”Ÿæˆè¦–è¦ºè§£é‡‹ç†±åŠ›åœ–"""
    # ä½¿ç”¨ Grad-CAM é¡¯ç¤ºé—œæ³¨å€åŸŸ
    attention_map = generate_gradcam(query_img, result_img)

    return {
        'attention_map': attention_map,
        'key_features': extract_key_features(attention_map),
        'explanation': generate_text_explanation(attention_map)
    }
```

---

## ğŸ“ˆ æ•ˆèƒ½å„ªåŒ–

### 1. ç´¢å¼•å„ªåŒ–
```python
# ä½¿ç”¨ IVF ç´¢å¼•åŠ é€Ÿå¤§è¦æ¨¡æœå°‹
nlist = 100  # èšé¡æ•¸é‡
quantizer = faiss.IndexFlatIP(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist)

# è¨“ç·´ç´¢å¼•
index.train(training_vectors)
index.add(vectors)

# æœå°‹æ™‚è¨­å®šæ¢æ¸¬ç¯„åœ
index.nprobe = 10  # æ¢æ¸¬ 10 å€‹èšé¡
```

### 2. GPU åŠ é€Ÿ
```python
# å°‡ç´¢å¼•ç§»è‡³ GPU
res = faiss.StandardGpuResources()
gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)

# æ‰¹æ¬¡æœå°‹
batch_results = gpu_index.search(batch_queries, k)
```

### 3. å¿«å–ç­–ç•¥
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_feature_extraction(image_hash):
    """å¿«å–ç‰¹å¾µæå–çµæœ"""
    return clip_extractor.extract_features(image_hash)
```

---

## ğŸ”§ é…ç½®æª”æ¡ˆ

```yaml
# config/search_config.yaml
clip:
  model_name: "ViT-B/32"  # æˆ– "ViT-L/14"
  device: "cuda"
  batch_size: 32

faiss:
  index_type: "IVFFlat"  # æˆ– "Flat", "HNSW"
  nlist: 100
  nprobe: 10
  dimension: 512

search:
  default_k: 5
  max_k: 20
  fusion_weights:
    clip: 0.7
    resnet: 0.3

similarity_threshold: 0.5
```

---

## ğŸ“Š è©•ä¼°æŒ‡æ¨™

### æœå°‹å“è³ªæŒ‡æ¨™
```python
# è©•ä¼°æœå°‹å¼•æ“æ•ˆèƒ½
metrics = {
    'precision@k': calculate_precision_at_k(results, ground_truth, k=5),
    'recall@k': calculate_recall_at_k(results, ground_truth, k=5),
    'mrr': calculate_mean_reciprocal_rank(results, ground_truth),
    'ndcg': calculate_ndcg(results, ground_truth)
}
```

### æ•ˆèƒ½æŒ‡æ¨™
```python
performance = {
    'index_build_time': measure_build_time(),
    'search_latency_p50': measure_search_latency(percentile=50),
    'search_latency_p95': measure_search_latency(percentile=95),
    'throughput': measure_throughput()
}
```

---

## ğŸ¯ ä½¿ç”¨å ´æ™¯

### å ´æ™¯ 1: æ‰¾ç›¸ä¼¼ STL æ¨¡å‹
```python
# ç”¨æˆ¶ä¸Šå‚³ä¸€å¼µåœ–ç‰‡ï¼Œæ‰¾æœ€ç›¸ä¼¼çš„ STL æ¨¡å‹
query_img = "user_upload.jpg"
results = clip_search_engine.search_by_image(query_img, k=10)

# çµæœï¼šTop 10 æœ€ç›¸ä¼¼çš„ STL æ¨¡å‹
```

### å ´æ™¯ 2: è‡ªç„¶èªè¨€æœå°‹
```python
# ç”¨æˆ¶è¼¸å…¥æ–‡å­—æè¿°
query = "éŠ€è‰²æˆ’æŒ‡ï¼Œæœ‰é‘½çŸ³ï¼Œåœ“å½¢"
results = clip_search_engine.search_by_text(query, k=5)

# çµæœï¼šç¬¦åˆæè¿°çš„ STL æ¨¡å‹
```

### å ´æ™¯ 3: å“è³ªæª¢æ¸¬
```python
# æ¯”å°ç”Ÿç”¢å“èˆ‡åŸå§‹ STL çš„ç›¸ä¼¼åº¦
production_img = "production_sample.jpg"
original_stl = "original_model.stl"

similarity = clip_search_engine.compare_similarity(
    production_img,
    original_stl
)

if similarity < 0.9:
    alert("å“è³ªç•°å¸¸ï¼ç›¸ä¼¼åº¦åƒ… {similarity*100}%")
```

### å ´æ™¯ 4: è‡ªå‹•åˆ†é¡
```python
# è‡ªå‹•å°‡æ–° STL åˆ†é¡åˆ°ç¾æœ‰é¡åˆ¥
new_stl = "new_model.stl"
top_class = clip_search_engine.classify(new_stl)

print(f"å»ºè­°åˆ†é¡: {top_class['name']} (ä¿¡å¿ƒåº¦: {top_class['confidence']})")
```

---

## ğŸš€ éƒ¨ç½²å»ºè­°

### é–‹ç™¼ç’°å¢ƒ
```bash
# æœ¬åœ°æ¸¬è©¦
python clip_faiss_engine.py --mode dev --device cpu
```

### ç”Ÿç”¢ç’°å¢ƒ
```bash
# GPU ä¼ºæœå™¨
python clip_faiss_engine.py --mode prod --device cuda --workers 4

# ä½¿ç”¨ Docker
docker run -it --gpus all \
  -v ./data:/data \
  -p 8082:8082 \
  stl-clip-search:latest
```

### å¾®æœå‹™æ¶æ§‹
```
â”œâ”€â”€ Feature Extraction Service (CLIP)
â”œâ”€â”€ FAISS Search Service
â”œâ”€â”€ API Gateway
â””â”€â”€ Web Frontend
```

---

## ğŸ“ ä¸‹ä¸€æ­¥è¡Œå‹•

### ç«‹å³å¯¦ä½œ (Priority 1)
- [ ] å®‰è£ CLIP ä¾è³´
- [ ] å‰µå»º CLIP ç‰¹å¾µæå–å™¨
- [ ] å»ºç«‹ CLIP-FAISS ç´¢å¼•
- [ ] å¯¦ä½œåŸºæœ¬æœå°‹ API

### çŸ­æœŸç›®æ¨™ (Priority 2)
- [ ] é–‹ç™¼æœå°‹ä»‹é¢
- [ ] å¯¦ä½œæ–‡å­—æœåœ–
- [ ] æ·»åŠ æ··åˆæœå°‹
- [ ] æ•ˆèƒ½æ¸¬è©¦èˆ‡å„ªåŒ–

### é•·æœŸç›®æ¨™ (Priority 3)
- [ ] å¤šå¼•æ“èåˆ
- [ ] è¦–è¦ºè§£é‡‹åŠŸèƒ½
- [ ] è‡ªå‹•æŸ¥è©¢æ“´å±•
- [ ] ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²

---

## ğŸ“š åƒè€ƒè³‡æº

### CLIP è³‡æº
- [OpenAI CLIP](https://github.com/openai/CLIP)
- [Hugging Face CLIP](https://huggingface.co/openai/clip-vit-base-patch32)
- [CLIP è«–æ–‡](https://arxiv.org/abs/2103.00020)

### FAISS è³‡æº
- [FAISS GitHub](https://github.com/facebookresearch/faiss)
- [FAISS æ–‡æª”](https://faiss.ai/)
- [FAISS æ•™å­¸](https://github.com/facebookresearch/faiss/wiki)

---

**ç‰ˆæœ¬**: v1.0
**ç‹€æ…‹**: ğŸ“‹ è¦åŠƒå®Œæˆï¼Œæº–å‚™å¯¦ä½œ
**é æœŸæ•ˆæœ**: ğŸš€ è¶…å¼·åœ–åƒç›¸ä¼¼æ€§æœå°‹å¼•æ“
