#!/usr/bin/env python3
"""
STL ç‰©ä»¶è­˜åˆ¥ - å®Œæ•´ç¶²é ä»‹é¢
åŒ…å«æª”æ¡ˆä¸Šå‚³ã€ç›¸æ©Ÿæ‹ç…§ã€æ¨¡å‹æ¸¬è©¦ç­‰å®Œæ•´åŠŸèƒ½
"""

# ç‰ˆæœ¬è³‡è¨Š
APP_VERSION = "1.0.0"
APP_BUILD_DATE = "2025-10-04"

from flask import Flask, render_template, request, jsonify, send_file, url_for, send_from_directory, session
from werkzeug.utils import secure_filename
import os
import re
import unicodedata
import cv2
import numpy as np
import base64
import io
from PIL import Image
from datetime import datetime, timedelta
from pathlib import Path
import json
import time
import random
import psutil

# å°å…¥ FAISS è­˜åˆ¥å¼•æ“
try:
    from faiss_recognition import predict_with_faiss, initialize_faiss
    FAISS_AVAILABLE = True
    print("âœ… FAISS è­˜åˆ¥å¼•æ“å¯ç”¨")
except ImportError as e:
    print(f"âš ï¸ FAISS è­˜åˆ¥å¼•æ“ä¸å¯ç”¨: {e}")
    FAISS_AVAILABLE = False
import subprocess
import threading
import shutil
import glob

# å¤šç”¨æˆ¶è¨“ç·´æ¨¡çµ„å·²ç§»é™¤ï¼Œæ”¹ç”¨ FAISS
# è³‡æ–™ç®¡ç†å’Œæ¨¡å‹ç®¡ç†æ¨¡çµ„å·²ç§»é™¤

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'web_uploads'
# è¨­ç½®æœ€å¤§ä¸Šå‚³å¤§å°ç‚º 500MBï¼ˆæ”¯æ´å¤§å‹ STL æª”æ¡ˆå’Œå¤šæª”æ¡ˆä¸Šå‚³ï¼‰
app.config['MAX_CONTENT_LENGTH'] = 500 * 1024 * 1024  # 500 MB

# ç¢ºä¿ä¸Šå‚³è³‡æ–™å¤¾å­˜åœ¨
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

# è¨»å†Š Blueprints
from blueprints import stl_bp, recognition_bp, training_bp, search_bp
app.register_blueprint(stl_bp)
app.register_blueprint(recognition_bp)
app.register_blueprint(training_bp)
app.register_blueprint(search_bp)

# æ”¯æ´ä¸­æ–‡çš„å®‰å…¨æª”åå‡½æ•¸
def safe_filename(filename):
    """
    æ”¯æ´ä¸­æ–‡å­—ç¬¦çš„å®‰å…¨æª”åå‡½æ•¸
    ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—ã€é»ã€åº•ç·šã€é€£å­—è™Ÿ
    """
    # åˆ†é›¢æª”åå’Œå‰¯æª”å
    name, ext = os.path.splitext(filename)

    # ç§»é™¤æˆ–æ›¿æ›å±éšªå­—ç¬¦ï¼Œä½†ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•¸å­—ã€åº•ç·šã€é€£å­—è™Ÿ
    # å…è¨±çš„å­—ç¬¦ï¼šä¸­æ–‡ã€è‹±æ–‡å­—æ¯ã€æ•¸å­—ã€åº•ç·šã€é€£å­—è™Ÿã€ç©ºæ ¼
    safe_name = re.sub(r'[^\w\s\-\u4e00-\u9fff]', '', name)

    # ç§»é™¤é–‹é ­å’Œçµå°¾çš„ç©ºç™½
    safe_name = safe_name.strip()

    # å¦‚æœæª”åç‚ºç©ºï¼ˆå¯èƒ½å…¨æ˜¯ç‰¹æ®Šå­—ç¬¦ï¼‰ï¼Œä½¿ç”¨æ™‚é–“æˆ³
    if not safe_name:
        from datetime import datetime
        safe_name = datetime.now().strftime('%Y%m%d_%H%M%S')

    # é‡æ–°çµ„åˆæª”åå’Œå‰¯æª”å
    return safe_name + ext
os.makedirs('static', exist_ok=True)
os.makedirs('static/results', exist_ok=True)
os.makedirs('training_stl', exist_ok=True)  # STL æª”æ¡ˆä¸Šå‚³è³‡æ–™å¤¾

# å…¨åŸŸè®Šæ•¸
model = None
model_loaded = False
model_info = {}
training_state_file = 'training_state.json'
class_names_file = 'class_names.json'

# å¤šç”¨æˆ¶è¨“ç·´ç®¡ç†
training_sessions = {}  # å­˜å„²å¤šå€‹è¨“ç·´æœƒè©± {session_id: {process, status, ...}}
training_lock = threading.Lock()

# è³‡æ–™ç®¡ç†å™¨ - å¯¦ä½œè³‡æ–™åº«å¯«å…¥åŠŸèƒ½
class DataManager:
    def __init__(self, db_path='system_data.db'):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–è³‡æ–™åº«è¡¨æ ¼"""
        import sqlite3
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # ä¸Šå‚³æ­·å²è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS upload_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    filename TEXT NOT NULL,
                    file_size INTEGER,
                    file_path TEXT,
                    client_ip TEXT,
                    user_agent TEXT,
                    status TEXT DEFAULT 'success'
                )
            ''')

            # è­˜åˆ¥æ­·å²è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS recognition_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    upload_id INTEGER,
                    timestamp TEXT NOT NULL,
                    method TEXT,
                    predicted_class TEXT,
                    confidence REAL,
                    inference_time REAL,
                    result_image_path TEXT,
                    success INTEGER,
                    error_message TEXT,
                    FOREIGN KEY (upload_id) REFERENCES upload_history (id)
                )
            ''')

            conn.commit()

    def add_upload_record(self, filename, file_size, file_path, client_ip=None, user_agent=None):
        """æ·»åŠ ä¸Šå‚³è¨˜éŒ„"""
        import sqlite3
        from datetime import datetime

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO upload_history (timestamp, filename, file_size, file_path, client_ip, user_agent)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (datetime.now().isoformat(), filename, file_size, file_path, client_ip, user_agent))
            conn.commit()
            return cursor.lastrowid

    def add_recognition_record(self, upload_id, method, predicted_class=None, confidence=None,
                              inference_time=None, result_image_path=None, success=True, error_message=None):
        """æ·»åŠ è­˜åˆ¥è¨˜éŒ„"""
        import sqlite3
        from datetime import datetime

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT INTO recognition_history
                (upload_id, timestamp, method, predicted_class, confidence, inference_time,
                 result_image_path, success, error_message)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (upload_id, datetime.now().isoformat(), method, predicted_class, confidence,
                  inference_time, result_image_path, 1 if success else 0, error_message))
            conn.commit()
            return cursor.lastrowid

    def get_statistics(self):
        """ç²å–ç³»çµ±çµ±è¨ˆè³‡æ–™"""
        import os
        import glob

        # çµ±è¨ˆ STL æª”æ¡ˆæ•¸é‡
        stl_files = glob.glob('STL/*.stl')
        stl_count = len(stl_files)

        # çµ±è¨ˆè¨“ç·´åœ–ç‰‡æ•¸é‡
        image_count = 0
        if os.path.exists('dataset'):
            for model_dir in os.listdir('dataset'):
                model_path = os.path.join('dataset', model_dir)
                if os.path.isdir(model_path):
                    images = glob.glob(os.path.join(model_path, '*.png'))
                    image_count += len(images)

        # çµ±è¨ˆä»Šæ—¥è­˜åˆ¥æ¬¡æ•¸ï¼ˆå¾æ•¸æ“šåº«ï¼‰
        recognition_count = 0
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                SELECT COUNT(*) FROM recognition_history
                WHERE date(timestamp) = date('now')
            ''')
            result = cursor.fetchone()
            recognition_count = result[0] if result else 0
            conn.close()
        except:
            recognition_count = 0

        return {
            'stl_count': stl_count,
            'image_count': image_count,
            'recognition_count': recognition_count,
            'class_count': stl_count  # é¡åˆ¥æ•¸é‡ç­‰æ–¼ STL æ¨¡å‹æ•¸é‡
        }

    def get_upload_history(self, **kwargs):
        return []

    def get_recognition_history(self, **kwargs):
        return []

    def get_training_history(self, **kwargs):
        return []

    def set_active_model(self, path):
        pass

    def scan_models(self):
        return []

    def get_model_summary(self):
        return {}

    def load_model(self, path):
        return False

    def get_current_model(self):
        return None

data_manager = DataManager()
model_manager = DataManager()  # ä¿æŒç›¸å®¹æ€§ï¼Œé›–ç„¶ä¸ä½¿ç”¨

def load_model():
    """è¼‰å…¥æ¨¡å‹ - ä½¿ç”¨ FAISS è­˜åˆ¥å¼•æ“"""
    global model, model_loaded, model_info

    print("â„¹ï¸ ç³»çµ±ä½¿ç”¨ FAISS è­˜åˆ¥å¼•æ“")

    if FAISS_AVAILABLE:
        # åˆå§‹åŒ– FAISS
        try:
            initialize_faiss()
            model_loaded = True
            model_info = {
                'method': 'FAISS',
                'loaded_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            print("âœ… FAISS è­˜åˆ¥å¼•æ“å·²åˆå§‹åŒ–")
            return True
        except Exception as e:
            print(f"âŒ FAISS åˆå§‹åŒ–å¤±æ•—: {e}")
            return False
    else:
        print("âŒ FAISS è­˜åˆ¥å¼•æ“ä¸å¯ç”¨")
        return False

def predict_with_faiss_wrapper(image_path):
    """FAISS é æ¸¬åŒ…è£å‡½æ•¸ï¼Œçµ±ä¸€è¼¸å‡ºæ ¼å¼"""
    try:
        result = predict_with_faiss(image_path)
        if not result:
            return {
                'predictions': [],
                'inference_time': 0,
                'result_image': None,
                'success': False,
                'error': 'FAISS é æ¸¬å¤±æ•—',
                'method': 'FAISS'
            }

        # è½‰æ›æ ¼å¼ä½¿å…¶èˆ‡ FAISS çµæœä¸€è‡´
        formatted_predictions = []
        detailed_results = result.get('detailed_results', [])

        for pred in result['predictions']:
            # å¾ detailed_results ç²å–è©²é¡åˆ¥æœ€ç›¸ä¼¼çš„åƒè€ƒåœ–ç‰‡
            reference_images = []
            class_name = pred['class_name']

            # æ‰¾å‡ºè©²é¡åˆ¥æœ€ç›¸ä¼¼çš„å‰8å€‹çµæœ
            class_matches = [d for d in detailed_results if d['class_name'] == class_name]
            class_matches = sorted(class_matches, key=lambda x: x['confidence'], reverse=True)[:8]

            for match in class_matches:
                ref_image_path = match.get('reference_image')
                if ref_image_path and os.path.exists(ref_image_path):
                    # è¤‡è£½åˆ° static è³‡æ–™å¤¾ä»¥ä¾¿ç¶²é å­˜å–
                    img_name = os.path.basename(ref_image_path)
                    static_filename = f"ref_{class_name}_{img_name}"
                    static_path = os.path.join('static', static_filename)

                    if not os.path.exists(static_path):
                        import shutil
                        try:
                            shutil.copy2(ref_image_path, static_path)
                        except Exception as e:
                            print(f"è¤‡è£½åƒè€ƒåœ–ç‰‡å¤±æ•—: {e}")
                            continue

                    reference_images.append({
                        'filename': img_name,
                        'url': f"/static/{static_filename}",
                        'confidence': match['confidence']
                    })

            # æŸ¥æ‰¾å°æ‡‰çš„ STL æª”æ¡ˆ
            stl_file = None
            stl_preview = None
            stl_path = os.path.join('STL', f"{class_name}.stl")
            if os.path.exists(stl_path):
                stl_file = f"/STL/{class_name}.stl"
                # ä½¿ç”¨è³‡æ–™é›†çš„ç¬¬ä¸€å¼µåœ–ä½œç‚º STL é è¦½
                dataset_dir = os.path.join('dataset', class_name)
                if os.path.exists(dataset_dir):
                    images = sorted([f for f in os.listdir(dataset_dir) if f.endswith(('.jpg', '.png'))])
                    if images:
                        preview_img = images[0]
                        static_preview = f"stl_preview_{class_name}_{preview_img}"
                        static_preview_path = os.path.join('static', static_preview)

                        if not os.path.exists(static_preview_path):
                            import shutil
                            try:
                                shutil.copy2(os.path.join(dataset_dir, preview_img), static_preview_path)
                            except Exception as e:
                                print(f"è¤‡è£½ STL é è¦½åœ–å¤±æ•—: {e}")

                        stl_preview = f"/static/{static_preview}"

            formatted_predictions.append({
                'class_id': pred['class_id'],
                'class_name': pred['class_name'],
                'confidence': pred['confidence'],
                'method': 'FAISS',
                'reference_images': reference_images,
                'stl_file': stl_file,
                'stl_preview': stl_preview
            })

        return {
            'predictions': formatted_predictions,
            'inference_time': result['inference_time'],
            'result_image': None,  # FAISS ä¸ç”¢ç”Ÿæ¨™è¨»åœ–ç‰‡
            'success': True,
            'method': 'FAISS'
        }

    except Exception as e:
        return {
            'predictions': [],
            'inference_time': 0,
            'result_image': None,
            'success': False,
            'error': f'FAISS é æ¸¬éŒ¯èª¤: {str(e)}',
            'method': 'FAISS'
        }

def predict_image(image_path, method='FAISS'):
    """é æ¸¬åœ–ç‰‡ - åªä½¿ç”¨ FAISS æ–¹æ³•"""
    global model, model_loaded

    # å¼·åˆ¶ä½¿ç”¨ FAISS
    if not FAISS_AVAILABLE:
        return {
            'predictions': [],
            'inference_time': 0,
            'result_image': None,
            'success': False,
            'error': 'FAISS è­˜åˆ¥å¼•æ“ä¸å¯ç”¨'
        }

    return predict_with_faiss_wrapper(image_path)

def get_dataset_samples():
    """å–å¾—æ•¸æ“šé›†æ¨£æœ¬"""
    samples = []

    # ä½¿ç”¨æ¨™æº–è³‡æ–™é›†ç›®éŒ„
    dataset_dir = "dataset"

    if not os.path.exists(dataset_dir):
        return samples

    if os.path.exists(dataset_dir):
        classes = [d for d in os.listdir(dataset_dir)
                   if os.path.isdir(os.path.join(dataset_dir, d))]

        for class_name in classes:
            class_dir = os.path.join(dataset_dir, class_name)
            images = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.png'))]

            if images:
                # éš¨æ©Ÿé¸æ“‡3å¼µåœ–ç‰‡
                selected_images = random.sample(images, min(3, len(images)))

                for img_name in selected_images:
                    img_path = os.path.join(class_dir, img_name)

                    # è¤‡è£½åˆ° static è³‡æ–™å¤¾ä»¥ä¾¿ç¶²é å­˜å–
                    static_filename = f"sample_{class_name}_{img_name}"
                    static_path = os.path.join('static', static_filename)

                    if not os.path.exists(static_path):
                        import shutil
                        shutil.copy2(img_path, static_path)

                    samples.append({
                        'class_name': class_name,
                        'filename': img_name,
                        'url': f"/static/{static_filename}",
                        'path': img_path
                    })

    return samples

def get_reference_images(class_name, count=3):
    """ç²å–æŒ‡å®šé¡åˆ¥çš„åƒè€ƒåœ–ç‰‡"""
    reference_images = []

    # ä½¿ç”¨æ¨™æº–è³‡æ–™é›†ç›®éŒ„
    dataset_dir = "dataset"

    if os.path.exists(dataset_dir):
        class_dir = os.path.join(dataset_dir, class_name)
        if os.path.exists(class_dir):
            images = [f for f in os.listdir(class_dir) if f.endswith(('.jpg', '.png'))]
            if images:
                # éš¨æ©Ÿé¸æ“‡æŒ‡å®šæ•¸é‡çš„åœ–ç‰‡
                selected_images = random.sample(images, min(count, len(images)))

                for img_name in selected_images:
                    img_path = os.path.join(class_dir, img_name)

                    # è¤‡è£½åˆ° static è³‡æ–™å¤¾ä»¥ä¾¿ç¶²é å­˜å–
                    static_filename = f"ref_{class_name}_{img_name}"
                    static_path = os.path.join('static', static_filename)

                    if not os.path.exists(static_path):
                        import shutil
                        shutil.copy2(img_path, static_path)

                    reference_images.append({
                        'class_name': class_name,
                        'filename': img_name,
                        'url': f"/static/{static_filename}",
                        'path': img_path
                    })

    return reference_images

# STL æª”æ¡ˆæœå‹™è·¯ç”±
@app.route('/STL/<path:filename>')
def serve_stl_file(filename):
    """æä¾› STL æª”æ¡ˆæœå‹™ï¼ˆæ”¯æ´ä¸­æ–‡æª”åï¼‰"""
    try:
        stl_dir = os.path.abspath('STL')
        return send_from_directory(stl_dir, filename, as_attachment=False)
    except Exception as e:
        print(f"STL æª”æ¡ˆæœå‹™éŒ¯èª¤: {e}")
        return jsonify({'error': f'æª”æ¡ˆä¸å­˜åœ¨: {filename}'}), 404

@app.route('/test_3d')
def test_3d():
    """3D é è¦½æ¸¬è©¦é é¢"""
    return render_template('test_3d.html')

@app.route('/')
def index():
    """ä¸»é é¢ - å„€è¡¨æ¿"""
    return render_template('dashboard/index.html',
                         version=APP_VERSION,
                         build_date=APP_BUILD_DATE)

@app.route('/legacy')
def legacy_index():
    """èˆŠç‰ˆä¸»é é¢ - ä¿ç•™å‘å¾Œå…¼å®¹"""
    return render_template('index_sidebar.html',
                         version=APP_VERSION,
                         build_date=APP_BUILD_DATE)

@app.route('/static/uploads/<filename>')
def uploaded_file(filename):
    """æœå‹™ä¸Šå‚³çš„æª”æ¡ˆ"""
    return send_from_directory(app.config['UPLOAD_FOLDER'], filename)

@app.route('/favicon.ico')
def favicon():
    """ç¶²ç«™åœ–æ¨™"""
    return send_from_directory('static', 'favicon.ico', mimetype='image/vnd.microsoft.icon')

@app.route('/web_uploads/<path:filename>', endpoint='web_uploads_file')
def serve_uploaded_file(filename):
    """æä¾›ä¸Šå‚³æª”æ¡ˆè¨ªå•"""
    return send_from_directory('web_uploads', filename)

@app.route('/dataset/<path:filename>')
def serve_dataset_file(filename):
    """æä¾› dataset è³‡æ–™å¤¾ä¸­çš„åœ–ç‰‡è¨ªå•"""
    return send_from_directory('dataset', filename)

@app.route('/simple')
def simple():
    """ç°¡åŒ–ç•Œé¢"""
    return render_template('index_simple.html')

@app.route('/advanced')
def advanced():
    """é€²éšåŠŸèƒ½é é¢"""
    return render_template('index_multi_upload.html')

@app.route('/single')
def single_upload():
    """å–®æª”æ¡ˆä¸Šå‚³é é¢"""
    return render_template('index.html')

@app.route('/recognition')
def recognition():
    """åœ–ç‰‡è­˜åˆ¥é é¢"""
    return render_template('recognition/index.html')

@app.route('/search')
def search():
    """æ™ºèƒ½æœå°‹é é¢"""
    return render_template('search/index.html')

@app.route('/training')
def training():
    """è¨“ç·´æ§åˆ¶é é¢"""
    return render_template('training/index.html')

@app.route('/stl')
def stl_list():
    """STL æª”æ¡ˆåˆ—è¡¨é é¢"""
    return render_template('stl/list.html')

@app.route('/stl/upload')
def stl_upload_page():
    """STL ä¸Šå‚³é é¢"""
    return render_template('stl/upload.html')

@app.route('/stl/generate')
def stl_generate_page():
    """STL ç”Ÿæˆåœ–ç‰‡é é¢"""
    return render_template('stl/generate.html')

@app.route('/api/model_status')
def model_status():
    """ç²å–æ¨¡å‹ç‹€æ…‹"""
    return jsonify({
        'loaded': model_loaded,
        'info': model_info if model_loaded else {}
    })

@app.route('/api/upload', methods=['POST'])
def upload_file():
    """æª”æ¡ˆä¸Šå‚³è™•ç† - æ”¯æ´å¤šæª”æ¡ˆå’Œæ¨¡å‹é¸æ“‡"""
    files = request.files.getlist('files') or [request.files.get('file')]
    files = [f for f in files if f and f.filename != '']
    recognition_method = 'FAISS'  # åªä½¿ç”¨ FAISS

    if not files:
        return jsonify({'success': False, 'error': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'})

    results = []
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    client_ip = request.remote_addr
    user_agent = request.headers.get('User-Agent', '')

    # å…è¨±çš„åœ–ç‰‡æ ¼å¼
    allowed_image_extensions = ('.png', '.jpg', '.jpeg', '.gif', '.bmp', '.webp')

    for i, file in enumerate(files):
        # æª¢æŸ¥æ˜¯å¦ç‚ºåœ–ç‰‡æª”æ¡ˆ
        if not file.filename.lower().endswith(allowed_image_extensions):
            results.append({
                'original_filename': file.filename,
                'error': f'ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ã€‚åƒ…æ¥å—åœ–ç‰‡æ ¼å¼ï¼šPNG, JPG, JPEG, GIF, BMP, WEBP',
                'success': False
            })
            continue

        if file and file.filename.lower().endswith(allowed_image_extensions):
            filename = safe_filename(file.filename)
            unique_filename = f"{timestamp}_{i:03d}_{filename}"
            filepath = os.path.join(app.config['UPLOAD_FOLDER'], unique_filename)
            file.save(filepath)

            file_size = os.path.getsize(filepath)

            # è¨˜éŒ„ä¸Šå‚³
            upload_id = data_manager.add_upload_record(
                filename=file.filename,
                file_size=file_size,
                file_path=filepath,
                client_ip=client_ip,
                user_agent=user_agent
            )

            # é€²è¡Œé æ¸¬ï¼ˆä½¿ç”¨é¸å®šçš„æ–¹æ³•ï¼‰
            result = predict_image(filepath, method=recognition_method)

            if result and result.get('success'):
                # è¨˜éŒ„è¾¨è­˜çµæœ
                predictions = result.get('predictions', [])
                if predictions:
                    pred = predictions[0]  # å–ç¬¬ä¸€å€‹é æ¸¬çµæœ
                    data_manager.add_recognition_record(
                        upload_id=upload_id,
                        method=recognition_method,
                        predicted_class=pred.get('class_name'),
                        confidence=pred.get('confidence'),
                        inference_time=result.get('inference_time'),
                        result_image_path=result.get('result_image'),
                        success=True
                    )
                else:
                    data_manager.add_recognition_record(
                        upload_id=upload_id,
                        method=recognition_method,
                        success=False,
                        error_message='æœªåµæ¸¬åˆ°ç‰©ä»¶'
                    )

                # æ‰å¹³åŒ–çµæœä»¥åŒ¹é…å‰ç«¯æ ¼å¼
                predictions = result.get('predictions', [])
                top_prediction = predictions[0] if predictions else None

                result_data = {
                    'original_filename': file.filename,
                    'saved_filename': unique_filename,
                    'original_image_url': f"/static/uploads/{unique_filename}",
                    'success': True,
                    'method': result.get('method', 'FAISS'),
                    'inference_time': result.get('inference_time', 0)
                }

                # æ·»åŠ é æ¸¬çµæœ
                if top_prediction:
                    result_data['class_id'] = top_prediction.get('class_id', -1)
                    result_data['class_name'] = top_prediction.get('class_name', 'Unknown')
                    result_data['confidence'] = top_prediction.get('confidence', 0)
                    result_data['top_k'] = predictions[:5]  # Top 5 çµæœ
                    result_data['reference_images'] = top_prediction.get('reference_images', [])
                    result_data['stl_file'] = top_prediction.get('stl_file')
                    result_data['stl_preview'] = top_prediction.get('stl_preview')

                results.append(result_data)
            else:
                # è¨˜éŒ„å¤±æ•—çš„è¾¨è­˜
                error_msg = result.get('error', 'é æ¸¬å¤±æ•—') if result else 'é æ¸¬å¤±æ•—'
                data_manager.add_recognition_record(
                    upload_id=upload_id,
                    method=recognition_method,
                    success=False,
                    error_message=error_msg
                )

                results.append({
                    'original_filename': file.filename,
                    'saved_filename': unique_filename,
                    'error': error_msg,
                    'success': False
                })
        else:
            results.append({
                'original_filename': file.filename if file else 'unknown',
                'error': 'ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼',
                'success': False
            })

    # è¨ˆç®—æˆåŠŸç‡
    successful = len([r for r in results if r['success']])
    total = len(results)

    return jsonify({
        'success': successful > 0,
        'total_files': total,
        'successful_files': successful,
        'failed_files': total - successful,
        'results': results
    })

@app.route('/api/camera_capture', methods=['POST'])
def camera_capture():
    """ç›¸æ©Ÿæ‹ç…§è™•ç†"""
    try:
        # ç²å–è³‡æ–™
        data = request.get_json()
        image_data = data['image'].split(',')[1]  # ç§»é™¤ data:image/jpeg;base64, å‰ç¶´
        recognition_method = 'FAISS'  # åªä½¿ç”¨ FAISS

        # è§£ç¢¼ä¸¦å„²å­˜
        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes))

        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"camera_{timestamp}.jpg"
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        image.save(filepath)

        # é€²è¡Œé æ¸¬ï¼ˆä½¿ç”¨é¸å®šçš„æ–¹æ³•ï¼‰
        result = predict_image(filepath, method=recognition_method)

        if result:
            return jsonify({
                'success': True,
                'filename': filename,
                'result': result
            })
        else:
            return jsonify({'success': False, 'error': 'é æ¸¬å¤±æ•—'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/test_sample/<class_name>/<filename>')
def test_sample(class_name, filename):
    """æ¸¬è©¦æ•¸æ“šé›†æ¨£æœ¬"""
    # ä½¿ç”¨æ¨™æº–è³‡æ–™é›†ç›®éŒ„
    img_path = os.path.join("dataset", class_name, filename)

    if os.path.exists(img_path):
        result = predict_image(img_path)

        if result:
            return jsonify({
                'success': True,
                'true_class': class_name,
                'result': result
            })
        else:
            return jsonify({'success': False, 'error': 'é æ¸¬å¤±æ•—'})
    else:
        return jsonify({'success': False, 'error': 'æ‰¾ä¸åˆ°åœ–ç‰‡'})

@app.route('/api/dataset_samples')
def dataset_samples():
    """ç²å–æ•¸æ“šé›†æ¨£æœ¬"""
    samples = get_dataset_samples()
    return jsonify({'samples': samples})

@app.route('/api/performance_test')
def performance_test():
    """æ•ˆèƒ½æ¸¬è©¦"""
    if not model_loaded:
        return jsonify({'success': False, 'error': 'æ¨¡å‹æœªè¼‰å…¥'})

    try:
        # æ‰¾ä¸€å¼µæ¸¬è©¦åœ–ç‰‡
        dataset_dir = "dataset"
        test_image = None

        for class_dir in os.listdir(dataset_dir):
            class_path = os.path.join(dataset_dir, class_dir)
            if os.path.isdir(class_path):
                images = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png'))]
                if images:
                    test_image = os.path.join(class_path, images[0])
                    break

        if not test_image:
            return jsonify({'success': False, 'error': 'æ‰¾ä¸åˆ°æ¸¬è©¦åœ–ç‰‡'})

        # åŸ·è¡Œå¤šæ¬¡æ¸¬è©¦
        times = []
        for i in range(10):
            start_time = time.time()
            results = model(test_image, verbose=False)
            end_time = time.time()
            times.append((end_time - start_time) * 1000)  # ms

        avg_time = np.mean(times)
        min_time = np.min(times)
        max_time = np.max(times)
        fps = 1000.0 / avg_time

        return jsonify({
            'success': True,
            'avg_time': round(avg_time, 1),
            'min_time': round(min_time, 1),
            'max_time': round(max_time, 1),
            'fps': round(fps, 1),
            'test_count': len(times)
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/batch_test')
def batch_test():
    """æ‰¹æ¬¡æ¸¬è©¦"""
    # FAISS ç³»çµ±ä¸éœ€è¦æª¢æŸ¥ model_loadedï¼Œå› ç‚ºå®ƒå§‹çµ‚å¯ç”¨
    if not FAISS_AVAILABLE:
        return jsonify({'success': False, 'error': 'FAISS å¼•æ“ä¸å¯ç”¨'})

    try:
        # ä½¿ç”¨ dataset è³‡æ–™å¤¾ï¼ˆåŸå§‹æ•¸æ“šé›†ï¼‰
        dataset_dir = "dataset"
        if not os.path.exists(dataset_dir):
            return jsonify({'success': False, 'error': f'è³‡æ–™é›†ç›®éŒ„ {dataset_dir} ä¸å­˜åœ¨'})

        classes = [d for d in os.listdir(dataset_dir)
                   if os.path.isdir(os.path.join(dataset_dir, d))]

        if not classes:
            return jsonify({'success': False, 'error': 'è³‡æ–™é›†ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•é¡åˆ¥'})

        results = {}
        total_correct = 0
        total_tested = 0

        for class_name in classes:
            class_dir = os.path.join(dataset_dir, class_name)
            # æ”¯æ´ .jpg å’Œ .png æ ¼å¼
            images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.jpg', '.png'))]

            if not images:
                continue

            # éš¨æ©Ÿé¸æ“‡10å¼µåœ–ç‰‡æ¸¬è©¦
            test_images = random.sample(images, min(10, len(images)))

            correct = 0
            for img_name in test_images:
                img_path = os.path.join(class_dir, img_name)

                # ä½¿ç”¨ FAISS é€²è¡Œé æ¸¬
                try:
                    result = predict_with_faiss(img_path)
                    if result and 'predictions' in result and result['predictions']:
                        predicted_class = result['predictions'][0]['class_name']
                        if predicted_class == class_name:
                            correct += 1
                except Exception as pred_error:
                    print(f"é æ¸¬å¤±æ•—: {img_path}, éŒ¯èª¤: {pred_error}")
                    continue

                total_tested += 1

            accuracy = correct / len(test_images) if test_images else 0
            total_correct += correct

            results[class_name] = {
                'correct': correct,
                'total': len(test_images),
                'accuracy': round(accuracy * 100, 1)
            }

        overall_accuracy = total_correct / total_tested if total_tested > 0 else 0

        return jsonify({
            'success': True,
            'results': results,
            'overall_accuracy': round(overall_accuracy * 100, 1),
            'accuracy': round(overall_accuracy * 100, 1),
            'map50': round(overall_accuracy * 100, 1),  # ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›ä½¿ç”¨æ™‚æ‡‰è¨ˆç®—çœŸæ­£çš„ mAP50
            'total_correct': total_correct,
            'total_tested': total_tested
        })

    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

# è¨“ç·´ç‹€æ…‹å…¨åŸŸè®Šæ•¸
training_process = None
training_status = {
    'is_training': False,
    'current_epoch': 0,
    'total_epochs': 0,
    'accuracy': 0,
    'log_lines': [],
    'start_time': None,
    'model_name': None,
    'pid': None
}

# è¼‰å…¥æŒä¹…åŒ–çš„è¨“ç·´ç‹€æ…‹
def load_training_state():
    """è¼‰å…¥è¨“ç·´ç‹€æ…‹"""
    global training_status, training_process
    try:
        if os.path.exists(training_state_file):
            with open(training_state_file, 'r', encoding='utf-8') as f:
                saved_state = json.load(f)
                training_status.update(saved_state)

            # æª¢æŸ¥æ˜¯å¦æœ‰æ­£åœ¨é€²è¡Œçš„è¨“ç·´ç¨‹åº
            if training_status.get('pid') and training_status.get('is_training'):
                try:
                    # æª¢æŸ¥ç¨‹åºæ˜¯å¦é‚„åœ¨é‹è¡Œ
                    import psutil
                    if psutil.pid_exists(training_status['pid']):
                        proc = psutil.Process(training_status['pid'])
                        if 'python' in proc.name().lower() and 'FAISS_training' in ' '.join(proc.cmdline()):
                            print(f"ç™¼ç¾æ­£åœ¨é‹è¡Œçš„è¨“ç·´ç¨‹åº (PID: {training_status['pid']})")
                            # é‡æ–°é€£æ¥åˆ°ç¾æœ‰ç¨‹åº (é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦æ›´è¤‡é›œçš„é‚è¼¯)
                        else:
                            training_status['is_training'] = False
                    else:
                        training_status['is_training'] = False
                except:
                    training_status['is_training'] = False

    except Exception as e:
        print(f"è¼‰å…¥è¨“ç·´ç‹€æ…‹å¤±æ•—: {e}")
        training_status['is_training'] = False

# ä¿å­˜è¨“ç·´ç‹€æ…‹
def save_training_state():
    """ä¿å­˜è¨“ç·´ç‹€æ…‹"""
    try:
        with open(training_state_file, 'w', encoding='utf-8') as f:
            json.dump(training_status, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"ä¿å­˜è¨“ç·´ç‹€æ…‹å¤±æ•—: {e}")

# è¼‰å…¥å’Œä¿å­˜åˆ†é¡åç¨±
def load_class_names():
    """è¼‰å…¥è‡ªè¨‚åˆ†é¡åç¨±"""
    try:
        if os.path.exists(class_names_file):
            with open(class_names_file, 'r', encoding='utf-8') as f:
                return json.load(f)
    except Exception as e:
        print(f"è¼‰å…¥åˆ†é¡åç¨±å¤±æ•—: {e}")
    return {}

def save_class_names(class_names):
    """ä¿å­˜è‡ªè¨‚åˆ†é¡åç¨±"""
    try:
        with open(class_names_file, 'w', encoding='utf-8') as f:
            json.dump(class_names, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        print(f"ä¿å­˜åˆ†é¡åç¨±å¤±æ•—: {e}")
        return False

# ç²å– STL æª”æ¡ˆçš„é è¨­åˆ†é¡åç¨±
def get_default_class_names():
    """ç²å– STL æª”æ¡ˆçš„é è¨­åˆ†é¡åç¨±"""
    class_names = {}

    # æƒæ STL è³‡æ–™å¤¾
    stl_files = glob.glob('STL/*.stl') + glob.glob('training_stl/*.stl')
    for stl_path in stl_files:
        filename = os.path.basename(stl_path)
        name_without_ext = os.path.splitext(filename)[0]
        class_names[filename] = name_without_ext

    # æƒæ dataset è³‡æ–™å¤¾ä¸­çš„é¡åˆ¥
    if os.path.exists('dataset'):
        for class_dir in os.listdir('dataset'):
            class_path = os.path.join('dataset', class_dir)
            if os.path.isdir(class_path):
                # æª¢æŸ¥æ˜¯å¦æœ‰å°æ‡‰çš„ STL æª”æ¡ˆ
                stl_file = f"{class_dir}.stl"
                if stl_file not in class_names:
                    class_names[stl_file] = class_dir

    return class_names

def get_system_info():
    """ç²å–ç³»çµ±è³‡è¨Š"""
    try:
        # CPU ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)

        # è¨˜æ†¶é«”è³‡è¨Š
        memory = psutil.virtual_memory()
        memory_used = f"{memory.used // (1024**3):.1f}GB"
        memory_total = f"{memory.total // (1024**3):.1f}GB"

        # ç£ç¢Ÿè³‡è¨Š
        disk = psutil.disk_usage('/')
        disk_used = f"{disk.used // (1024**3):.1f}GB"
        disk_total = f"{disk.total // (1024**3):.1f}GB"

        # ç¶²è·¯è³‡è¨Š
        network = psutil.net_io_counters()
        network_sent = f"{network.bytes_sent // (1024**2):.0f}MB"
        network_recv = f"{network.bytes_recv // (1024**2):.0f}MB"

        # GPU è³‡è¨Š (å˜—è©¦ä½¿ç”¨ nvidia-smi)
        gpu_info = None
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu',
                                   '--format=csv,noheader,nounits'], capture_output=True, text=True)
            if result.returncode == 0:
                gpu_data = result.stdout.strip().split(', ')
                if len(gpu_data) >= 4:
                    gpu_info = {
                        'utilization': gpu_data[0],
                        'memory_used': gpu_data[1],
                        'memory_total': gpu_data[2],
                        'memory_percent': f"{float(gpu_data[1]) / float(gpu_data[2]) * 100:.1f}",
                        'temperature': gpu_data[3]
                    }
        except:
            pass

        # ç³»çµ±æº«åº¦ (å„ªå…ˆç²å– CPU æ ¸å¿ƒæº«åº¦)
        temperature = None
        try:
            if hasattr(psutil, "sensors_temperatures"):
                temps = psutil.sensors_temperatures()
                if temps:
                    # å„ªå…ˆé¸æ“‡ coretemp (CPU æ ¸å¿ƒæº«åº¦)
                    if 'coretemp' in temps and temps['coretemp']:
                        temperature = temps['coretemp'][0].current
                    elif 'acpitz' in temps and temps['acpitz']:
                        temperature = temps['acpitz'][0].current
                    else:
                        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç‰¹å®šæº«åº¦æ„Ÿæ¸¬å™¨ï¼Œä½¿ç”¨ç¬¬ä¸€å€‹å¯ç”¨çš„
                        for name, entries in temps.items():
                            if entries:
                                temperature = entries[0].current
                                break
        except:
            pass

        return {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_used': memory_used,
            'memory_total': memory_total,
            'disk_percent': disk.percent,
            'disk_used': disk_used,
            'disk_total': disk_total,
            'network_sent': network_sent,
            'network_recv': network_recv,
            'gpu_info': gpu_info,
            'temperature': temperature
        }
    except Exception as e:
        return {'error': str(e)}

@app.route('/api/system_status')
def system_status():
    """ç³»çµ±ç‹€æ…‹API"""
    status = get_system_info()

    # è¨ˆç®—å·²è¨“ç·´çš„ STL æª”æ¡ˆæ•¸é‡ï¼ˆdataset è³‡æ–™å¤¾ä¸­çš„å­è³‡æ–™å¤¾æ•¸é‡ï¼‰
    try:
        from pathlib import Path
        dataset_path = Path('dataset')
        if dataset_path.exists():
            # è¨ˆç®—æœ‰åœ–ç‰‡çš„å­è³‡æ–™å¤¾æ•¸é‡
            trained_stl_count = sum(1 for item in dataset_path.iterdir()
                                   if item.is_dir() and any(item.glob('*.png')))
            status['trained_stl_count'] = trained_stl_count
        else:
            status['trained_stl_count'] = 0
    except Exception as e:
        print(f"è¨ˆç®—è¨“ç·´ STL æ•¸é‡éŒ¯èª¤: {e}")
        status['trained_stl_count'] = 0

    return jsonify({'success': True, 'status': status})

@app.route('/api/start_training', methods=['POST'])
def start_training():
    """é–‹å§‹å¤šæ¨¡å‹è¨“ç·´API - æ”¯æŒå¤šç”¨æˆ¶ä¸¦ç™¼è¨“ç·´"""
    global training_sessions

    try:
        config = request.get_json()
        client_ip = request.remote_addr

        # ç”Ÿæˆå”¯ä¸€æœƒè©±ID
        session_id = f"{client_ip}_{int(time.time() * 1000)}"

        print(f"ğŸ“¥ æ”¶åˆ°è¨“ç·´è«‹æ±‚ - IP: {client_ip}, Session: {session_id}")

        # æª¢æŸ¥ç•¶å‰æ´»èºè¨“ç·´æ•¸é‡ - åªå…è¨±ä¸€å€‹äººè¨“ç·´
        with training_lock:
            active_sessions = [s for s in training_sessions.values() if s['status']['is_training']]

            # å¦‚æœæœ‰å…¶ä»–ç”¨æˆ¶æ­£åœ¨è¨“ç·´ï¼Œæ‹’çµ•æ–°çš„è¨“ç·´è«‹æ±‚
            if len(active_sessions) > 0:
                # æª¢æŸ¥æ˜¯å¦æ˜¯åŒä¸€å€‹ç”¨æˆ¶
                current_training = active_sessions[0]
                if current_training['client_ip'] != client_ip:
                    # å…¶ä»–ç”¨æˆ¶æ­£åœ¨è¨“ç·´ï¼Œæ‹’çµ•
                    other_user_ip = current_training['client_ip']
                    error_msg = f"âš ï¸ ç³»çµ±æ­£å¿™ï¼šç”¨æˆ¶ {other_user_ip} æ­£åœ¨è¨“ç·´ä¸­"
                    print(error_msg)
                    return jsonify({
                        'success': False,
                        'error': 'è¨“ç·´ç³»çµ±æ­£åœ¨ä½¿ç”¨ä¸­',
                        'message': f'å¦ä¸€å€‹ç”¨æˆ¶ï¼ˆ{other_user_ip}ï¼‰æ­£åœ¨é€²è¡Œè¨“ç·´ï¼Œè«‹ç¨å¾Œå†è©¦',
                        'other_user': other_user_ip,
                        'retry_after': 'è«‹ç­‰å¾…ç•¶å‰è¨“ç·´å®Œæˆ'
                    })
                else:
                    # åŒä¸€å€‹ç”¨æˆ¶ï¼Œå…è¨±é‡æ–°è¨“ç·´ï¼ˆå…ˆæ¸…ç†èˆŠæœƒè©±ï¼‰
                    print(f"ç”¨æˆ¶ {client_ip} é‡æ–°é–‹å§‹è¨“ç·´ï¼Œæ¸…ç†èˆŠæœƒè©±")
                    # ä¸é˜»æ­¢ï¼Œè®“å…¶ç¹¼çºŒå‰µå»ºæ–°æœƒè©±

        # æª¢æŸ¥è¨“ç·´æ–¹æ³•é¸æ“‡ - åªæ”¯æ´ FAISS
        training_methods_list = config.get('training_methods', ['faiss'])
        # å¼·åˆ¶åªä½¿ç”¨ FAISS
        faiss_enabled = True

        # è¨“ç·´æ–¹æ³•å­—å…¸ï¼ˆç”¨æ–¼ç‹€æ…‹è¿½è¸ªï¼‰
        training_methods = {
            'faiss': True
        }

        # å‰µå»ºæ–°çš„è¨“ç·´æœƒè©±
        training_status = {
            'is_training': True,
            'current_epoch': 0,
            'total_epochs': config.get('faiss_config', {}).get('epochs', 50) if faiss_enabled else 1,
            'accuracy': 0,
            'log_lines': [],
            'training_methods': training_methods,
            'faiss_enabled': faiss_enabled,
            'faiss_enabled': faiss_enabled,
            'start_time': time.time(),
            'client_ip': client_ip
        }

        # æ·»åŠ å¤šç”¨æˆ¶è­¦å‘Šåˆ°æ—¥èªŒ
        training_status['log_lines'].append('ğŸš€ é–‹å§‹å¤šæ¨¡å‹è¨“ç·´ç³»çµ±åˆå§‹åŒ–...')
        with training_lock:
            active_count = len([s for s in training_sessions.values() if s['status']['is_training']])
            if active_count > 0:
                training_status['log_lines'].append(f'âš ï¸ ç³»çµ±ç•¶å‰æœ‰ {active_count} å€‹è¨“ç·´ä»»å‹™æ­£åœ¨é‹è¡Œ')
                training_status['log_lines'].append('ğŸ’¡ æç¤ºï¼šå¤šå€‹è¨“ç·´ä»»å‹™æœƒå…±äº«GPUè³‡æºï¼Œå¯èƒ½å½±éŸ¿è¨“ç·´é€Ÿåº¦')

        # é¡¯ç¤ºè¨“ç·´æ–¹æ³• - åªä½¿ç”¨ FAISS
        training_status['log_lines'].append('ğŸ¯ ä½¿ç”¨è¨“ç·´æ–¹æ³•: FAISS ç‰¹å¾µç´¢å¼•')

        # å­˜å„²æœƒè©±
        with training_lock:
            training_sessions[session_id] = {
                'client_ip': client_ip,
                'status': training_status,
                'process': None,
                'created_at': time.time()
            }

        # å•Ÿå‹•è¨“ç·´ä»»å‹™ - è‡ªå‹•ç”Ÿæˆåœ–ç‰‡ + FAISS è¨“ç·´
        training_status['log_lines'].append('ğŸš€ é–‹å§‹å®Œæ•´è¨“ç·´æµç¨‹...')

        # å®Œæ•´è¨“ç·´æµç¨‹ï¼š1. æª¢æŸ¥/ç”Ÿæˆåœ–ç‰‡ â†’ 2. FAISS è¨“ç·´
        def run_faiss_training():
            # è¼”åŠ©å‡½æ•¸ï¼šæ·»åŠ æ—¥èªŒä¸¦æ‰“å°åˆ°æ§åˆ¶å°ï¼ˆå¸¶æ™‚é–“æˆ³è¨˜ï¼‰
            def add_log(message):
                import datetime
                timestamp = datetime.datetime.now().strftime('%H:%M:%S')
                timestamped_message = f"[{timestamp}] {message}"
                with training_lock:
                    if session_id in training_sessions:
                        training_sessions[session_id]['status']['log_lines'].append(timestamped_message)
                print(f"[è¨“ç·´æ—¥èªŒ] {timestamped_message}")

            try:
                import subprocess
                import os

                with training_lock:
                    if session_id in training_sessions:
                        status = training_sessions[session_id]['status']

                # éšæ®µ 1: æª¢æŸ¥è¨“ç·´åœ–ç‰‡
                add_log('ğŸ“‹ éšæ®µ 1/2: æª¢æŸ¥è¨“ç·´åœ–ç‰‡...')

                # æƒæ STL æª”æ¡ˆ
                stl_files = glob.glob('STL/*.stl')
                need_generate = False
                total_images = 0

                for stl_path in stl_files:
                    stl_name = os.path.splitext(os.path.basename(stl_path))[0]
                    image_dir = os.path.join('dataset', stl_name)

                    if not os.path.exists(image_dir):
                        need_generate = True
                        add_log(f'   âš ï¸ {stl_name}: åœ–ç‰‡è³‡æ–™å¤¾ä¸å­˜åœ¨')
                    else:
                        images = [f for f in os.listdir(image_dir) if f.endswith('.png')]
                        total_images += len(images)
                        if len(images) == 0:
                            need_generate = True
                            add_log(f'   âš ï¸ {stl_name}: ç„¡åœ–ç‰‡')
                        else:
                            add_log(f'   âœ… {stl_name}: {len(images)} å¼µåœ–ç‰‡')

                # å¦‚æœéœ€è¦ç”Ÿæˆåœ–ç‰‡ï¼ˆåªåœ¨å®Œå…¨æ²’æœ‰åœ–ç‰‡æ™‚æ‰ç”Ÿæˆï¼‰
                if need_generate:
                    add_log('ğŸ“¸ åµæ¸¬åˆ°éƒ¨åˆ†æ¨¡å‹ç„¡åœ–ç‰‡ï¼Œé–‹å§‹è‡ªå‹•ç”Ÿæˆ...')
                    add_log(f'   æ‰¾åˆ° {len(stl_files)} å€‹ STL æ¨¡å‹')

                    # ä½¿ç”¨ Popen å¯¦æ™‚è®€å–åœ–ç‰‡ç”Ÿæˆè¼¸å‡º
                    process = subprocess.Popen(
                        ['python', '-u', 'generate_images_color.py'],
                        stdout=subprocess.PIPE,
                        stderr=subprocess.STDOUT,
                        text=True,
                        bufsize=1
                    )

                    # å¯¦æ™‚è®€å–ä¸¦é¡¯ç¤ºè¼¸å‡º
                    for line in iter(process.stdout.readline, ''):
                        if not line:
                            break
                        line = line.strip()
                        if line:
                            add_log(f'   {line}')

                    process.wait()

                    if process.returncode == 0:
                        add_log('âœ… åœ–ç‰‡ç”Ÿæˆå®Œæˆ')
                    else:
                        add_log(f'âŒ åœ–ç‰‡ç”Ÿæˆå¤±æ•—ï¼Œè¿”å›ç¢¼: {process.returncode}')
                        with training_lock:
                            if session_id in training_sessions:
                                training_sessions[session_id]['status']['is_training'] = False
                        return
                else:
                    add_log(f'âœ… æ‰€æœ‰æ¨¡å‹å·²æœ‰åœ–ç‰‡ï¼Œç¸½è¨ˆ {total_images} å¼µï¼Œç›´æ¥é–‹å§‹è¨“ç·´')

                # éšæ®µ 2: FAISS è¨“ç·´
                add_log('ğŸ“‹ éšæ®µ 2/2: FAISS ç´¢å¼•å»ºç«‹...')

                # ä½¿ç”¨ Popen å¯¦æ™‚è®€å–è¼¸å‡º
                import re
                process = subprocess.Popen(
                    ['python', '-u', 'faiss_recognition.py'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1
                )

                # å¯¦æ™‚è®€å–è¼¸å‡ºä¸¦è§£æé€²åº¦
                total_images = 0
                processed_images = 0

                for line in iter(process.stdout.readline, ''):
                    if not line:
                        break

                    line = line.strip()
                    if line:
                        add_log(line)

                        # è§£æç¸½åœ–ç‰‡æ•¸
                        total_match = re.search(r'ç¸½å…±éœ€è¦è™•ç†\s+(\d+)\s+å¼µåœ–ç‰‡', line)
                        if total_match:
                            total_images = int(total_match.group(1))

                        # è§£æç•¶å‰é€²åº¦
                        progress_match = re.search(r'é€²åº¦:\s+(\d+)/(\d+)\s+å¼µåœ–ç‰‡\s+\((\d+\.?\d*)%\)', line)
                        if progress_match and total_images > 0:
                            processed_images = int(progress_match.group(1))
                            progress_percent = float(progress_match.group(3))

                            # æ›´æ–°è¨“ç·´ç‹€æ…‹çš„é€²åº¦
                            with training_lock:
                                if session_id in training_sessions:
                                    training_sessions[session_id]['status']['current_epoch'] = processed_images
                                    training_sessions[session_id]['status']['total_epochs'] = total_images
                                    training_sessions[session_id]['status']['progress_percent'] = progress_percent

                process.wait()

                if process.returncode == 0:
                    add_log('âœ… FAISS è¨“ç·´å®Œæˆ')

                    # è¨“ç·´å®Œæˆå ±å‘Š
                    add_log('')
                    add_log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
                    add_log('â•‘          ğŸ“Š è¨“ç·´å®Œæˆå ±å‘Š                      â•‘')
                    add_log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')
                    add_log('')

                    # æª¢æŸ¥ STL æª”æ¡ˆæ•¸é‡ vs è¨“ç·´çš„æ¨¡å‹æ•¸é‡
                    stl_files = glob.glob('STL/*.stl')
                    stl_count = len(stl_files)

                    # çµ±è¨ˆè³‡æ–™é›†åœ–ç‰‡æ•¸é‡
                    total_dataset_images = 0
                    for stl_path in stl_files:
                        stl_name = os.path.splitext(os.path.basename(stl_path))[0]
                        image_dir = os.path.join('dataset', stl_name)
                        if os.path.exists(image_dir):
                            images = [f for f in os.listdir(image_dir) if f.endswith('.png')]
                            total_dataset_images += len(images)

                    # æª¢æŸ¥ FAISS è¨“ç·´çš„æ¨¡å‹æ•¸é‡
                    try:
                        import pickle
                        import datetime

                        if os.path.exists('faiss_labels.pkl'):
                            with open('faiss_labels.pkl', 'rb') as f:
                                faiss_data = pickle.load(f)
                                trained_classes = len(faiss_data.get('classes', []))
                                total_features = len(faiss_data.get('labels', []))

                            # ç²å–ç´¢å¼•æ–‡ä»¶å¤§å°
                            index_size = 0
                            if os.path.exists('faiss_features.index'):
                                index_size = os.path.getsize('faiss_features.index') / (1024 * 1024)  # MB

                            add_log('ğŸ“¦ è³‡æ–™é›†çµ±è¨ˆï¼š')
                            add_log(f'   â””â”€ STL æ¨¡å‹æ•¸é‡: {stl_count} å€‹')
                            add_log(f'   â””â”€ è¨“ç·´åœ–ç‰‡ç¸½æ•¸: {total_dataset_images} å¼µ')
                            add_log(f'   â””â”€ å¹³å‡æ¯æ¨¡å‹: {total_dataset_images // stl_count if stl_count > 0 else 0} å¼µåœ–ç‰‡')
                            add_log('')

                            add_log('ğŸ¯ FAISS ç´¢å¼•è³‡è¨Šï¼š')
                            add_log(f'   â””â”€ è¨“ç·´é¡åˆ¥æ•¸: {trained_classes} å€‹')
                            add_log(f'   â””â”€ ç‰¹å¾µå‘é‡æ•¸: {total_features} å€‹')
                            add_log(f'   â””â”€ ç´¢å¼•æª”æ¡ˆå¤§å°: {index_size:.2f} MB')
                            add_log(f'   â””â”€ ç´¢å¼•é¡å‹: IndexFlatIP (å…§ç©ç›¸ä¼¼åº¦)')
                            add_log('')

                            # æª¢æŸ¥å®Œæ•´æ€§
                            if trained_classes < stl_count:
                                missing_count = stl_count - trained_classes
                                add_log('âš ï¸  è¨“ç·´å®Œæ•´æ€§æª¢æŸ¥ï¼š')
                                add_log(f'   â”œâ”€ STL æª”æ¡ˆç¸½æ•¸: {stl_count} å€‹')
                                add_log(f'   â”œâ”€ å·²è¨“ç·´æ¨¡å‹æ•¸: {trained_classes} å€‹')
                                add_log(f'   â””â”€ æœªè¨“ç·´æ¨¡å‹æ•¸: {missing_count} å€‹')
                                add_log('')

                                # æ‰¾å‡ºæœªè¨“ç·´çš„æ¨¡å‹
                                trained_names = set(faiss_data.get('classes', []))
                                stl_names = set(os.path.splitext(os.path.basename(f))[0] for f in stl_files)
                                missing_models = stl_names - trained_names

                                if missing_models:
                                    add_log('âŒ æœªè¨“ç·´çš„æ¨¡å‹ï¼š')
                                    for model in sorted(missing_models):
                                        add_log(f'   â€¢ {model}')
                                    add_log('')
                                    add_log('ğŸ’¡ å»ºè­°ï¼šè«‹é‡æ–°è¨“ç·´ä»¥åŒ…å«æ‰€æœ‰æ¨¡å‹')

                                with training_lock:
                                    if session_id in training_sessions:
                                        training_sessions[session_id]['status']['training_incomplete'] = True
                            else:
                                add_log('âœ… è¨“ç·´å®Œæ•´æ€§æª¢æŸ¥ï¼š')
                                add_log(f'   â””â”€ æ‰€æœ‰ {trained_classes} å€‹æ¨¡å‹å·²æˆåŠŸè¨“ç·´')
                                add_log('')

                                with training_lock:
                                    if session_id in training_sessions:
                                        training_sessions[session_id]['status']['training_incomplete'] = False

                            # è¨“ç·´ç¸½çµ
                            add_log('ğŸ“ˆ è¨“ç·´ç¸½çµï¼š')
                            add_log(f'   â””â”€ è¨“ç·´ç‹€æ…‹: {"âš ï¸ ä¸å®Œæ•´" if trained_classes < stl_count else "âœ… å®Œæ•´"}')
                            add_log(f'   â””â”€ è¨“ç·´æ–¹æ³•: FAISS ç‰¹å¾µç´¢å¼•')
                            add_log(f'   â””â”€ ç‰¹å¾µæå–: ResNet50 (ImageNet é è¨“ç·´)')
                            add_log(f'   â””â”€ æœç´¢æ–¹å¼: K-è¿‘é„°æŠ•ç¥¨æ©Ÿåˆ¶')
                            add_log('')

                            # ä½¿ç”¨èªªæ˜
                            add_log('ğŸ“ ä½¿ç”¨èªªæ˜ï¼š')
                            add_log('   â”œâ”€ 1. å‰å¾€é¦–é ä¸Šå‚³åœ–ç‰‡é€²è¡Œè­˜åˆ¥')
                            add_log('   â”œâ”€ 2. ç³»çµ±æœƒè¿”å›æœ€ç›¸ä¼¼çš„æ¨¡å‹å’Œç½®ä¿¡åº¦')
                            add_log('   â”œâ”€ 3. "æŠ•ç¥¨æ•¸" è¡¨ç¤º K å€‹æœ€ç›¸ä¼¼åœ–ç‰‡ä¸­æœ‰å¹¾å€‹å±¬æ–¼è©²é¡åˆ¥')
                            add_log('   â””â”€ 4. æŠ•ç¥¨æ•¸è¶Šé«˜ï¼Œè­˜åˆ¥çµæœè¶Šå¯é ')
                            add_log('')

                            add_log('â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—')
                            add_log('â•‘          ğŸ‰ è¨“ç·´å®Œæˆï¼ç³»çµ±å·²å°±ç·’              â•‘')
                            add_log('â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•')

                            # ç”Ÿæˆè¨“ç·´å ±å‘Šæ–‡ä»¶
                            try:
                                report_filename = f'training_report_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
                                with open(report_filename, 'w', encoding='utf-8') as rf:
                                    rf.write('â•' * 60 + '\n')
                                    rf.write('          ğŸ“Š FAISS è¨“ç·´å®Œæˆå ±å‘Š\n')
                                    rf.write('â•' * 60 + '\n\n')
                                    rf.write(f'å ±å‘Šç”Ÿæˆæ™‚é–“: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}\n\n')

                                    rf.write('â”' * 60 + '\n')
                                    rf.write('ğŸ“¦ è³‡æ–™é›†çµ±è¨ˆ\n')
                                    rf.write('â”' * 60 + '\n')
                                    rf.write(f'STL æ¨¡å‹æ•¸é‡: {stl_count} å€‹\n')
                                    rf.write(f'è¨“ç·´åœ–ç‰‡ç¸½æ•¸: {total_dataset_images} å¼µ\n')
                                    rf.write(f'å¹³å‡æ¯æ¨¡å‹: {total_dataset_images // stl_count if stl_count > 0 else 0} å¼µåœ–ç‰‡\n\n')

                                    rf.write('â”' * 60 + '\n')
                                    rf.write('ğŸ¯ FAISS ç´¢å¼•è³‡è¨Š\n')
                                    rf.write('â”' * 60 + '\n')
                                    rf.write(f'è¨“ç·´é¡åˆ¥æ•¸: {trained_classes} å€‹\n')
                                    rf.write(f'ç‰¹å¾µå‘é‡æ•¸: {total_features} å€‹\n')
                                    rf.write(f'ç´¢å¼•æª”æ¡ˆå¤§å°: {index_size:.2f} MB\n')
                                    rf.write(f'ç´¢å¼•é¡å‹: IndexFlatIP (å…§ç©ç›¸ä¼¼åº¦)\n\n')

                                    rf.write('â”' * 60 + '\n')
                                    rf.write('âœ… è¨“ç·´å®Œæ•´æ€§æª¢æŸ¥\n')
                                    rf.write('â”' * 60 + '\n')
                                    if trained_classes < stl_count:
                                        rf.write(f'ç‹€æ…‹: âš ï¸ è¨“ç·´ä¸å®Œæ•´\n')
                                        rf.write(f'STL æª”æ¡ˆç¸½æ•¸: {stl_count} å€‹\n')
                                        rf.write(f'å·²è¨“ç·´æ¨¡å‹æ•¸: {trained_classes} å€‹\n')
                                        rf.write(f'æœªè¨“ç·´æ¨¡å‹æ•¸: {stl_count - trained_classes} å€‹\n\n')
                                        if missing_models:
                                            rf.write('æœªè¨“ç·´çš„æ¨¡å‹åˆ—è¡¨:\n')
                                            for model in sorted(missing_models):
                                                rf.write(f'  âŒ {model}\n')
                                    else:
                                        rf.write(f'ç‹€æ…‹: âœ… è¨“ç·´å®Œæ•´\n')
                                        rf.write(f'æ‰€æœ‰ {trained_classes} å€‹æ¨¡å‹å·²æˆåŠŸè¨“ç·´\n')
                                    rf.write('\n')

                                    rf.write('â”' * 60 + '\n')
                                    rf.write('ğŸ“ˆ è¨“ç·´ç¸½çµ\n')
                                    rf.write('â”' * 60 + '\n')
                                    rf.write(f'è¨“ç·´ç‹€æ…‹: {"âš ï¸ ä¸å®Œæ•´" if trained_classes < stl_count else "âœ… å®Œæ•´"}\n')
                                    rf.write(f'è¨“ç·´æ–¹æ³•: FAISS ç‰¹å¾µç´¢å¼•\n')
                                    rf.write(f'ç‰¹å¾µæå–: ResNet50 (ImageNet é è¨“ç·´)\n')
                                    rf.write(f'æœç´¢æ–¹å¼: K-è¿‘é„°æŠ•ç¥¨æ©Ÿåˆ¶\n\n')

                                    rf.write('â”' * 60 + '\n')
                                    rf.write('ğŸ“ ä½¿ç”¨èªªæ˜\n')
                                    rf.write('â”' * 60 + '\n')
                                    rf.write('1. å‰å¾€é¦–é ä¸Šå‚³åœ–ç‰‡é€²è¡Œè­˜åˆ¥\n')
                                    rf.write('2. ç³»çµ±æœƒè¿”å›æœ€ç›¸ä¼¼çš„æ¨¡å‹å’Œç½®ä¿¡åº¦\n')
                                    rf.write('3. "æŠ•ç¥¨æ•¸" è¡¨ç¤º K å€‹æœ€ç›¸ä¼¼åœ–ç‰‡ä¸­æœ‰å¹¾å€‹å±¬æ–¼è©²é¡åˆ¥\n')
                                    rf.write('4. æŠ•ç¥¨æ•¸è¶Šé«˜ï¼Œè­˜åˆ¥çµæœè¶Šå¯é \n\n')

                                    rf.write('â•' * 60 + '\n')
                                    rf.write('          ğŸ‰ è¨“ç·´å®Œæˆï¼ç³»çµ±å·²å°±ç·’\n')
                                    rf.write('â•' * 60 + '\n')

                                add_log(f'ğŸ“„ è¨“ç·´å ±å‘Šå·²å„²å­˜è‡³: {report_filename}')
                            except Exception as e:
                                add_log(f'âš ï¸ ç”Ÿæˆå ±å‘Šæ–‡ä»¶å¤±æ•—: {str(e)}')

                        else:
                            add_log('âš ï¸ æ‰¾ä¸åˆ° FAISS æ¨™ç±¤æª”æ¡ˆ')
                    except Exception as e:
                        add_log(f'âš ï¸ å®Œæ•´æ€§æª¢æŸ¥éŒ¯èª¤: {str(e)}')

                    with training_lock:
                        if session_id in training_sessions:
                            training_sessions[session_id]['status']['is_training'] = False
                else:
                    add_log(f'âŒ FAISS è¨“ç·´å¤±æ•—: {result.stderr}')
                    with training_lock:
                        if session_id in training_sessions:
                            training_sessions[session_id]['status']['is_training'] = False

            except Exception as e:
                add_log(f'âŒ éŒ¯èª¤: {str(e)}')
                with training_lock:
                    if session_id in training_sessions:
                        training_sessions[session_id]['status']['is_training'] = False

        training_thread = threading.Thread(target=run_faiss_training)
        training_thread.daemon = True
        training_thread.start()

        return jsonify({
            'success': True,
            'message': 'å¤šæ¨¡å‹è¨“ç·´å·²å•Ÿå‹•',
            'session_id': session_id,
            'active_sessions': len([s for s in training_sessions.values() if s['status']['is_training']])
        })

    except Exception as e:
        import traceback
        print(f"âŒ è¨“ç·´å•Ÿå‹•å¤±æ•—: {e}")
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

def monitor_training_process():
    """ç›£æ§è¨“ç·´éç¨‹"""
    global training_process, training_status

    if not training_process:
        return

    import re
    last_heartbeat = time.time()
    heartbeat_interval = 10  # æ¯10ç§’ç™¼é€ä¸€æ¬¡å¿ƒè·³è¨Šæ¯

    try:
        while True:
            # éé˜»å¡è®€å–
            line = training_process.stdout.readline()

            if not line:
                # æª¢æŸ¥é€²ç¨‹æ˜¯å¦çµæŸ
                if training_process.poll() is not None:
                    break

                # å¦‚æœé•·æ™‚é–“æ²’æœ‰è¼¸å‡ºï¼Œç™¼é€å¿ƒè·³è¨Šæ¯
                current_time = time.time()
                if current_time - last_heartbeat > heartbeat_interval:
                    current_epoch = training_status.get('current_epoch', 0)
                    total_epochs = training_status.get('total_epochs', 0)
                    if current_epoch > 0:
                        progress = (current_epoch / total_epochs * 100) if total_epochs > 0 else 0
                        training_status['log_lines'].append(f'ğŸ’“ è¨“ç·´é‹è¡Œä¸­... Epoch {current_epoch}/{total_epochs} ({progress:.1f}%)')
                    else:
                        training_status['log_lines'].append('ğŸ’“ è¨“ç·´åˆå§‹åŒ–ä¸­ï¼Œè«‹ç¨å€™...')
                    last_heartbeat = current_time

                time.sleep(0.1)
                continue

            line = line.strip()
            if line:
                # é‡ç½®å¿ƒè·³è¨ˆæ™‚å™¨
                last_heartbeat = time.time()

                # æ·»åŠ æ‰€æœ‰éç©ºæ—¥èªŒè¡Œ
                training_status['log_lines'].append(line)
                print(f"[è¨“ç·´æ—¥èªŒ] {line}")  # åŒæ™‚è¼¸å‡ºåˆ°æ§åˆ¶å°

                # è§£æè¨“ç·´é€²åº¦ï¼ˆæ›´å¼·å¥çš„æ­£å‰‡è¡¨é”å¼åŒ¹é…ï¼‰
                epoch_pattern = r'(\d+)/(\d+)'
                if 'epoch' in line.lower():
                    try:
                        match = re.search(epoch_pattern, line)
                        if match:
                            current = int(match.group(1))
                            total = int(match.group(2))
                            training_status['current_epoch'] = current
                            training_status['total_epochs'] = total
                    except Exception as e:
                        print(f"è§£æ epoch å¤±æ•—: {e}")

                # è§£ææå¤±å€¼
                if 'loss' in line.lower():
                    try:
                        loss_match = re.search(r'loss[:\s]+(\d+\.\d+)', line.lower())
                        if loss_match:
                            training_status['loss'] = float(loss_match.group(1))
                    except:
                        pass

                # è§£ææº–ç¢ºç‡
                if 'acc' in line.lower() or 'accuracy' in line.lower():
                    try:
                        acc_match = re.search(r'acc(?:uracy)?[:\s]+(\d+\.?\d*)', line.lower())
                        if acc_match:
                            training_status['accuracy'] = float(acc_match.group(1))
                    except:
                        pass

                # è§£æ mAP ä¸¦æ·»åŠ é‡é»æ¨™è¨»
                if 'map' in line.lower():
                    try:
                        map_match = re.search(r'map(?:@?[\d.]*)?[:\s]+(\d+\.\d+)', line.lower())
                        if map_match:
                            training_status['mAP'] = float(map_match.group(1))
                            training_status['log_lines'].append(f"ğŸ“Š æª¢æ¸¬åˆ° mAP æŒ‡æ¨™: {training_status['mAP']:.4f}")
                    except:
                        pass

                # ä¿æŒæœ€æ–°çš„300è¡Œæ—¥èªŒï¼ˆå¢åŠ å®¹é‡ä»¥é¡¯ç¤ºæ›´å¤šå…§å®¹ï¼‰
                if len(training_status['log_lines']) > 300:
                    training_status['log_lines'] = training_status['log_lines'][-300:]

        # è¨“ç·´å®Œæˆ
        training_status['is_training'] = False
        training_status['log_lines'].append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')
        training_status['log_lines'].append('ğŸ è¨“ç·´å·²å®Œæˆ')
        training_status['log_lines'].append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')
        print("è¨“ç·´é€²ç¨‹å·²çµæŸ")

    except Exception as e:
        training_status['is_training'] = False
        training_status['log_lines'].append(f"âŒ ç›£æ§éŒ¯èª¤: {str(e)}")
        print(f"ç›£æ§éŒ¯èª¤: {e}")

@app.route('/api/training_status')
def get_training_status():
    """ç²å–è¨“ç·´ç‹€æ…‹ - å¤šç”¨æˆ¶ç‰ˆæœ¬"""
    global training_sessions, training_status

    client_ip = request.remote_addr

    # æŸ¥æ‰¾è©²ç”¨æˆ¶çš„æœ€æ–°æœƒè©±
    with training_lock:
        user_sessions = [(sid, s) for sid, s in training_sessions.items() if s['client_ip'] == client_ip]

        if user_sessions:
            # æŒ‰å‰µå»ºæ™‚é–“æ’åºï¼Œå–æœ€æ–°çš„
            user_sessions.sort(key=lambda x: x[1]['created_at'], reverse=True)
            session_id, session_data = user_sessions[0]
            status = session_data['status'].copy()

            # æ·»åŠ å¤šç”¨æˆ¶ä¿¡æ¯
            active_sessions = [s for s in training_sessions.values() if s['status']['is_training']]
            status['active_sessions_count'] = len(active_sessions)
            status['session_id'] = session_id

            return jsonify(status)
        else:
            # æ²’æœ‰æœƒè©±ï¼Œè¿”å›é»˜èªç‹€æ…‹
            return jsonify(training_status)

@app.route('/api/stop_training', methods=['POST'])
def stop_training():
    """åœæ­¢è¨“ç·´"""
    global training_process, training_status

    try:
        if training_process:
            training_process.terminate()
            training_process = None

        training_status['is_training'] = False
        training_status['log_lines'].append("è¨“ç·´å·²æ‰‹å‹•åœæ­¢")

        return jsonify({'success': True, 'message': 'è¨“ç·´å·²åœæ­¢'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/reset_training_status', methods=['POST'])
def reset_training_status():
    """é‡ç½®è¨“ç·´ç‹€æ…‹ï¼ˆä¸åœæ­¢è¨“ç·´é€²ç¨‹ï¼‰"""
    global training_sessions, training_status

    try:
        # ç²å–å®¢æˆ¶ç«¯ IP
        client_ip = request.remote_addr

        # æ¸…é™¤è©²å®¢æˆ¶ç«¯çš„æ‰€æœ‰è¨“ç·´æœƒè©±
        sessions_to_remove = []
        for session_id, session_data in training_sessions.items():
            if session_data.get('client_ip') == client_ip:
                sessions_to_remove.append(session_id)

        for session_id in sessions_to_remove:
            del training_sessions[session_id]

        # é‡ç½®è¨“ç·´ç‹€æ…‹
        training_status['is_training'] = False
        training_status['current_epoch'] = 0
        training_status['total_epochs'] = 0
        training_status['log_lines'] = []

        return jsonify({
            'success': True,
            'message': 'è¨“ç·´ç‹€æ…‹å·²é‡ç½®',
            'cleared_sessions': len(sessions_to_remove)
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

# ==================== æ¨¡å‹ç®¡ç† API ====================

@app.route('/api/list_models')
def list_models():
    """åˆ—å‡ºæ‰€æœ‰å·²è¨“ç·´çš„æ¨¡å‹ï¼ˆFAISSï¼‰"""
    try:
        models = []

        # æª¢æŸ¥ FAISS æ¨¡å‹æª”æ¡ˆ
        faiss_index_path = Path('faiss_features.index')
        faiss_labels_path = Path('faiss_labels.pkl')

        if faiss_index_path.exists() and faiss_labels_path.exists():
            import pickle

            # ç²å–æª”æ¡ˆä¿¡æ¯
            index_stat = faiss_index_path.stat()
            labels_stat = faiss_labels_path.stat()
            total_size = index_stat.st_size + labels_stat.st_size

            # è®€å–æ¨™ç±¤ç²å–é¡åˆ¥ä¿¡æ¯
            try:
                with open(faiss_labels_path, 'rb') as f:
                    labels_data = pickle.load(f)
                    # labels_data æ˜¯å­—å…¸ï¼ŒåŒ…å« 'labels' å’Œ 'classes' éµ
                    if isinstance(labels_data, dict):
                        class_names = labels_data.get('classes', [])
                        label_array = labels_data.get('labels', [])
                        num_classes = len(class_names)
                        num_features = len(label_array)
                    else:
                        # å¦‚æœæ˜¯èˆŠæ ¼å¼ï¼ˆlistï¼‰
                        num_classes = len(set(labels_data))
                        num_features = len(labels_data)
                        class_names = sorted(set(labels_data))
            except:
                num_classes = 0
                num_features = 0
                class_names = []

            # FAISS æ¨¡å‹å§‹çµ‚è™•æ–¼æ´»å‹•ç‹€æ…‹ï¼ˆåªè¦æª”æ¡ˆå­˜åœ¨ï¼‰
            is_active = FAISS_AVAILABLE

            model_info = {
                'name': 'FAISS è­˜åˆ¥æ¨¡å‹',
                'path': str(faiss_index_path),
                'type': 'FAISS',
                'size': total_size,
                'created_time': datetime.fromtimestamp(index_stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                'num_classes': num_classes,
                'num_features': num_features,
                'class_names': class_names,
                'is_active': is_active,
                'files': {
                    'index': str(faiss_index_path),
                    'labels': str(faiss_labels_path),
                    'index_size': index_stat.st_size,
                    'labels_size': labels_stat.st_size
                }
            }
            models.append(model_info)

        return jsonify({'success': True, 'models': models})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/load_model', methods=['POST'])
def switch_model():
    """åˆ‡æ›åˆ°æŒ‡å®šçš„æ¨¡å‹"""
    global faiss_engine

    try:
        data = request.json
        model_path = data.get('model_path')

        if not model_path or not Path(model_path).exists():
            return jsonify({'success': False, 'error': 'æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨'})

        # FAISS å·²ç§»é™¤ï¼Œåªä½¿ç”¨ FAISS
        return jsonify({'success': False, 'error': 'ç³»çµ±å·²æ”¹ç”¨ FAISSï¼Œä¸æ”¯æ´ FAISS æ¨¡å‹è¼‰å…¥'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/model_info')
def model_info():
    """ç²å–æ¨¡å‹è©³ç´°è³‡è¨Š"""
    try:
        model_path = request.args.get('path')

        if not model_path or not Path(model_path).exists():
            return jsonify({'success': False, 'error': 'æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨'})

        model_path = Path(model_path)
        stat = model_path.stat()
        run_dir = model_path.parent.parent

        info = {
            'name': f"{run_dir.name}/{model_path.name}",
            'path': str(model_path),
            'size': stat.st_size,
            'created_time': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
        }

        # è®€å–è¨“ç·´é…ç½®
        args_file = run_dir / 'args.yaml'
        if args_file.exists():
            try:
                import yaml
                with open(args_file, 'r') as f:
                    args = yaml.safe_load(f)
                    info['epochs'] = args.get('epochs')
                    info['batch_size'] = args.get('batch')
                    info['image_size'] = args.get('imgsz')
            except:
                pass

        # è®€å–è¨“ç·´çµæœ
        results_file = run_dir / 'results.csv'
        if results_file.exists():
            try:
                import pandas as pd
                df = pd.read_csv(results_file)
                if not df.empty:
                    last_row = df.iloc[-1]
                    # å˜—è©¦è®€å–å„ç¨®å¯èƒ½çš„åˆ—å
                    for col in df.columns:
                        col_lower = col.strip().lower()
                        if 'map' in col_lower or 'map50' in col_lower:
                            info['map'] = f"{float(last_row[col]):.4f}"
                        elif 'precision' in col_lower:
                            info['precision'] = f"{float(last_row[col]):.4f}"
                        elif 'recall' in col_lower:
                            info['recall'] = f"{float(last_row[col]):.4f}"
            except:
                pass

        return jsonify({'success': True, 'info': info})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/export_model', methods=['GET', 'POST'])
def export_model():
    """åŒ¯å‡ºæ¨¡å‹æª”æ¡ˆ"""
    try:
        # æ”¯æ´ GET å’Œ POST å…©ç¨®æ–¹å¼ï¼ŒåŒæ™‚æ”¯æ´ path å’Œ model_path åƒæ•¸å
        if request.method == 'POST':
            data = request.json
            model_path = data.get('model_path') or data.get('path') if data else None
        else:
            model_path = request.args.get('path') or request.args.get('model_path')

        if not model_path:
            return jsonify({'success': False, 'error': 'æœªæŒ‡å®šæ¨¡å‹è·¯å¾‘'})

        model_path = Path(model_path)

        if not model_path.exists():
            return jsonify({'success': False, 'error': f'æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨: {model_path}'})

        return send_file(str(model_path), as_attachment=True)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/delete_model', methods=['POST'])
def delete_model():
    """åˆªé™¤æ¨¡å‹æª”æ¡ˆ"""
    try:
        data = request.json
        model_path = data.get('model_path')

        if not model_path:
            return jsonify({'success': False, 'error': 'æœªæŒ‡å®šæ¨¡å‹è·¯å¾‘'})

        model_path = Path(model_path)

        # æª¢æŸ¥æ˜¯å¦æ˜¯ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹
        current_model_path = None
        try:
            if hasattr(app, 'faiss_engine') and app.faiss_engine:
                current_model_path = str(getattr(app.faiss_engine, 'ckpt_path', None))
        except:
            pass

        if str(model_path) == current_model_path:
            return jsonify({'success': False, 'error': 'ç„¡æ³•åˆªé™¤æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹'})

        if model_path.exists():
            model_path.unlink()
            return jsonify({'success': True, 'message': 'æ¨¡å‹å·²åˆªé™¤'})
        else:
            return jsonify({'success': False, 'error': 'æ¨¡å‹æª”æ¡ˆä¸å­˜åœ¨'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/check_training_conflict', methods=['POST'])
def check_training_conflict():
    """æª¢æŸ¥è¨“ç·´è¼¸å‡ºç›®éŒ„æ˜¯å¦å·²å­˜åœ¨"""
    try:
        data = request.json
        project_name = data.get('project_name', 'faiss_model')

        # æª¢æŸ¥æ˜¯å¦æœ‰åŒåçš„è¨“ç·´ç›®éŒ„
        base_path = Path('./runs/detect')
        conflicts = []

        if not base_path.exists():
            return jsonify({'success': True, 'has_conflict': False})

        # æª¢æŸ¥ç²¾ç¢ºåŒ¹é…å’Œå¸¶æ•¸å­—å¾Œç¶´çš„åŒ¹é…
        for item in base_path.iterdir():
            if item.is_dir():
                # æª¢æŸ¥æ˜¯å¦æ˜¯åŒåæˆ–åŒååŠ æ•¸å­—
                if item.name == project_name or item.name.startswith(project_name):
                    # æª¢æŸ¥æ˜¯å¦æœ‰æ¬Šé‡æª”æ¡ˆ
                    weights_dir = item / 'weights'
                    if weights_dir.exists():
                        best_pt = weights_dir / 'best.pt'
                        last_pt = weights_dir / 'last.pt'

                        if best_pt.exists() or last_pt.exists():
                            stat = best_pt.stat() if best_pt.exists() else last_pt.stat()
                            conflicts.append({
                                'name': item.name,
                                'path': str(item),
                                'size': stat.st_size,
                                'created_time': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                                'has_best': best_pt.exists(),
                                'has_last': last_pt.exists()
                            })

        if conflicts:
            return jsonify({
                'success': True,
                'has_conflict': True,
                'conflicts': conflicts
            })
        else:
            return jsonify({
                'success': True,
                'has_conflict': False
            })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/generate_dataset', methods=['POST'])
def generate_dataset():
    """é‡æ–°ç”Ÿæˆè³‡æ–™é›†"""
    try:
        # åŸ·è¡Œåœ–ç‰‡ç”Ÿæˆè…³æœ¬
        result = subprocess.run(['python', 'generate_images.py'],
                              capture_output=True, text=True, timeout=300)

        if result.returncode == 0:
            # è¨ˆç®—ç”Ÿæˆçš„åœ–ç‰‡æ•¸é‡
            image_count = 0
            dataset_dir = "dataset"

            if os.path.exists(dataset_dir):
                for class_dir in os.listdir(dataset_dir):
                    class_path = os.path.join(dataset_dir, class_dir)
                    if os.path.isdir(class_path):
                        images = [f for f in os.listdir(class_path) if f.endswith(('.jpg', '.png'))]
                        image_count += len(images)

            return jsonify({'success': True, 'image_count': image_count})
        else:
            return jsonify({'success': False, 'error': result.stderr})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/validate_model', methods=['POST'])
def validate_model():
    """é©—è­‰æ¨¡å‹"""
    try:
        # ä½¿ç”¨ç¾æœ‰çš„æ‰¹æ¬¡æ¸¬è©¦åŠŸèƒ½
        return batch_test()

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/validate_models', methods=['POST'])
def validate_models():
    """é©—è­‰æ‰€æœ‰æ¨¡å‹ï¼ˆFAISS ç³»çµ±åªæœ‰ä¸€å€‹æ¨¡å‹ï¼Œæ‰€ä»¥ç­‰åŒæ–¼ validate_modelï¼‰"""
    try:
        return batch_test()
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/upload_stl', methods=['POST'])
def upload_stl():
    """STLæª”æ¡ˆä¸Šå‚³API - æª¢æ¸¬é‡è¤‡ä¸¦æä¾›é¸é …"""
    import time
    start_time = time.time()

    try:
        files = request.files.getlist('stl_files')
        force_upload = request.form.get('force_upload', 'false').lower() == 'true'

        if not files:
            return jsonify({'success': False, 'error': 'æ²’æœ‰é¸æ“‡æª”æ¡ˆ'})

        print(f"ğŸ“¤ æ”¶åˆ° {len(files)} å€‹ STL æª”æ¡ˆä¸Šå‚³è«‹æ±‚")

        # ç¢ºä¿ STL ç›®éŒ„å­˜åœ¨
        stl_dir = 'STL'
        os.makedirs(stl_dir, exist_ok=True)

        # å…ˆå°‡æ‰€æœ‰æª”æ¡ˆä¿å­˜åˆ°è‡¨æ™‚ä½ç½®ä»¥é¿å…è¨˜æ†¶é«”å•é¡Œ
        import tempfile
        temp_files = []

        for file in files:
            if file and file.filename.lower().endswith('.stl'):
                # ä¿å­˜åˆ°è‡¨æ™‚æª”æ¡ˆ
                temp_fd, temp_path = tempfile.mkstemp(suffix='.stl')
                os.close(temp_fd)
                file.save(temp_path)

                temp_files.append({
                    'original_name': file.filename,
                    'safe_name': safe_filename(file.filename),
                    'temp_path': temp_path,
                    'size': os.path.getsize(temp_path)
                })
            else:
                # æ¸…ç†å·²ä¿å­˜çš„è‡¨æ™‚æª”æ¡ˆ
                for tf in temp_files:
                    try:
                        os.remove(tf['temp_path'])
                    except:
                        pass
                return jsonify({'success': False, 'error': f'æª”æ¡ˆ {file.filename} ä¸æ˜¯STLæ ¼å¼'})

        # æª¢æŸ¥é‡è¤‡æª”æ¡ˆ
        duplicate_files = []
        new_files = []

        for temp_file in temp_files:
            filename = temp_file['safe_name']
            filepath = os.path.join(stl_dir, filename)

            if os.path.exists(filepath):
                # ç²å–ç¾æœ‰æª”æ¡ˆè³‡è¨Š
                existing_size = os.path.getsize(filepath)
                existing_time = datetime.fromtimestamp(os.path.getmtime(filepath)).strftime('%Y-%m-%d %H:%M:%S')

                duplicate_files.append({
                    'name': filename,
                    'original_name': temp_file['original_name'],
                    'existing_size': existing_size,
                    'existing_time': existing_time,
                    'temp_path': temp_file['temp_path'],
                    'new_size': temp_file['size']
                })
            else:
                new_files.append({
                    'name': filename,
                    'original_name': temp_file['original_name'],
                    'temp_path': temp_file['temp_path'],
                    'size': temp_file['size']
                })

        # å¦‚æœæœ‰é‡è¤‡æª”æ¡ˆä¸”æœªå¼·åˆ¶ä¸Šå‚³ï¼Œæ¸…ç†è‡¨æ™‚æª”æ¡ˆä¸¦è¿”å›è­¦å‘Š
        if duplicate_files and not force_upload:
            # æ¸…ç†æ‰€æœ‰è‡¨æ™‚æª”æ¡ˆ
            for tf in temp_files:
                try:
                    os.remove(tf['temp_path'])
                except:
                    pass

            return jsonify({
                'success': False,
                'warning': 'duplicate_detected',
                'duplicate_files': [{
                    'name': f['name'],
                    'original_name': f['original_name'],
                    'existing_size': f['existing_size'],
                    'existing_time': f['existing_time']
                } for f in duplicate_files],
                'new_files_count': len(new_files),
                'duplicate_count': len(duplicate_files),
                'message': f'ç™¼ç¾ {len(duplicate_files)} å€‹é‡è¤‡æª”æ¡ˆ'
            })

        # åŸ·è¡Œä¸Šå‚³ï¼ˆå¾è‡¨æ™‚æª”æ¡ˆç§»å‹•åˆ°ç›®æ¨™ä½ç½®ï¼‰
        uploaded_files = []

        # ä¸Šå‚³æ–°æª”æ¡ˆ
        for file_info in new_files:
            filename = file_info['name']
            filepath = os.path.join(stl_dir, filename)

            # å¾è‡¨æ™‚ä½ç½®ç§»å‹•åˆ°ç›®æ¨™ä½ç½®
            import shutil
            shutil.move(file_info['temp_path'], filepath)

            uploaded_files.append({
                'name': filename,
                'original_name': file_info['original_name'],
                'size': os.path.getsize(filepath),
                'path': filepath,
                'status': 'new'
            })

        # å¦‚æœå¼·åˆ¶ä¸Šå‚³ï¼Œè¦†è“‹é‡è¤‡æª”æ¡ˆ
        if force_upload and duplicate_files:
            for file_info in duplicate_files:
                filename = file_info['name']
                filepath = os.path.join(stl_dir, filename)

                # åˆªé™¤èˆŠçš„åœ–ç‰‡è³‡æ–™å¤¾
                model_name = os.path.splitext(filename)[0]
                dataset_dir = os.path.join('dataset', model_name)
                if os.path.exists(dataset_dir):
                    import shutil
                    shutil.rmtree(dataset_dir)

                # å¾è‡¨æ™‚ä½ç½®ç§»å‹•åˆ°ç›®æ¨™ä½ç½®ï¼ˆè¦†è“‹ï¼‰
                shutil.move(file_info['temp_path'], filepath)

                uploaded_files.append({
                    'name': filename,
                    'original_name': file_info['original_name'],
                    'size': os.path.getsize(filepath),
                    'path': filepath,
                    'status': 'replaced'
                })

        response = {
            'success': True,
            'files': uploaded_files,
            'count': len(uploaded_files),
            'new_count': len([f for f in uploaded_files if f['status'] == 'new']),
            'replaced_count': len([f for f in uploaded_files if f['status'] == 'replaced'])
        }

        if force_upload and duplicate_files:
            response['message'] = f'æˆåŠŸä¸Šå‚³ {len(new_files)} å€‹æ–°æª”æ¡ˆï¼Œè¦†è“‹ {len(duplicate_files)} å€‹é‡è¤‡æª”æ¡ˆ'
        else:
            response['message'] = f'æˆåŠŸä¸Šå‚³ {len(uploaded_files)} å€‹æª”æ¡ˆ'

        return jsonify(response)

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/generate_from_stl', methods=['POST'])
def generate_from_stl():
    """å¾STLæª”æ¡ˆç”Ÿæˆè¨“ç·´è³‡æ–™é›† - ä½¿ç”¨èƒŒæ™¯ä»»å‹™é¿å…è¶…æ™‚"""
    try:
        data = request.get_json()
        stl_files = data.get('stl_files', [])

        if not stl_files:
            return jsonify({'success': False, 'error': 'æ²’æœ‰STLæª”æ¡ˆ'})

        # æª¢æŸ¥ STL æª”æ¡ˆæ˜¯å¦éƒ½åœ¨ STL ç›®éŒ„ä¸­
        # æ³¨æ„ï¼šæª”æ¡ˆåç¨±å¯èƒ½å·²è¢« safe_filename æ¸…ç†é
        missing_files = []
        valid_files = []

        for stl_file in stl_files:
            # é¦–å…ˆå˜—è©¦åŸå§‹æª”å
            stl_path = os.path.join('STL', stl_file)
            if os.path.exists(stl_path):
                valid_files.append(stl_file)
                continue

            # å¦‚æœåŸå§‹æª”åä¸å­˜åœ¨ï¼Œå˜—è©¦æ¸…ç†å¾Œçš„æª”å
            safe_name = safe_filename(stl_file)
            safe_path = os.path.join('STL', safe_name)
            if os.path.exists(safe_path):
                valid_files.append(safe_name)
                continue

            # å…©è€…éƒ½ä¸å­˜åœ¨ï¼Œæ¨™è¨˜ç‚ºç¼ºå¤±
            missing_files.append(stl_file)

        if missing_files:
            return jsonify({
                'success': False,
                'error': f'ä»¥ä¸‹ STL æª”æ¡ˆä¸å­˜åœ¨æ–¼ STL ç›®éŒ„: {", ".join(missing_files)}ã€‚è«‹æª¢æŸ¥æª”æ¡ˆåç¨±æ˜¯å¦æ­£ç¢ºã€‚'
            })

        # ä½¿ç”¨æ‰¾åˆ°çš„æœ‰æ•ˆæª”æ¡ˆåç¨±
        stl_files = valid_files

        # è¨ˆç®—åœ–ç‰‡æ•¸é‡
        images_per_stl = 360  # æ¯å€‹STLç”Ÿæˆ360å¼µåœ–ç‰‡
        total_images = len(stl_files) * images_per_stl

        # ä½¿ç”¨èƒŒæ™¯é€²ç¨‹åŸ·è¡Œåœ–ç‰‡ç”Ÿæˆï¼Œé¿å…è¶…æ™‚
        import subprocess
        import threading

        # å‰µå»ºæ—¥èªŒæ–‡ä»¶
        log_file = 'training_logs/image_generation.log'
        os.makedirs('training_logs', exist_ok=True)

        def run_generation():
            global training_status
            try:
                # æ›´æ–°è¨“ç·´ç‹€æ…‹
                training_status['is_training'] = True
                training_status['current_epoch'] = 0
                training_status['total_epochs'] = len(stl_files)
                training_status['log_lines'] = []
                training_status['log_lines'].append(f'ğŸ“¦ é–‹å§‹ç”Ÿæˆ {len(stl_files)} å€‹æ¨¡å‹çš„åœ–ç‰‡è³‡æ–™é›†')
                training_status['log_lines'].append(f'ğŸ“Š é è¨ˆç”Ÿæˆ {total_images} å¼µè¨“ç·´åœ–ç‰‡')

                # ä½¿ç”¨ Popen å¯¦æ™‚è®€å–è¼¸å‡º
                process = subprocess.Popen(
                    ['python', 'generate_images_color.py'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1
                )

                # å¯¦æ™‚è®€å–ä¸¦è¨˜éŒ„è¼¸å‡º
                with open(log_file, 'w', encoding='utf-8') as f:
                    for line in process.stdout:
                        line = line.strip()
                        if line:
                            # å¯«å…¥æ—¥èªŒæ–‡ä»¶
                            f.write(line + '\n')
                            f.flush()

                            # æ›´æ–°è¨“ç·´ç‹€æ…‹
                            training_status['log_lines'].append(line)

                            # é™åˆ¶æ—¥èªŒè¡Œæ•¸
                            if len(training_status['log_lines']) > 200:
                                training_status['log_lines'] = training_status['log_lines'][-200:]

                            # è§£æé€²åº¦ä¿¡æ¯
                            if 'Processing' in line or 'è™•ç†' in line:
                                # å˜—è©¦è§£ææ¨¡å‹åºè™Ÿ
                                import re
                                match = re.search(r'(\d+)/(\d+)', line)
                                if match:
                                    current = int(match.group(1))
                                    training_status['current_epoch'] = current

                process.wait()

                # å®Œæˆå¾Œæ›´æ–°ç‹€æ…‹
                if process.returncode == 0:
                    training_status['log_lines'].append('âœ… åœ–ç‰‡ç”Ÿæˆå®Œæˆï¼')
                    training_status['is_training'] = False
                else:
                    training_status['log_lines'].append(f'âŒ åœ–ç‰‡ç”Ÿæˆå¤±æ•—ï¼Œè¿”å›ç¢¼: {process.returncode}')
                    training_status['is_training'] = False

            except Exception as e:
                training_status['log_lines'].append(f'âŒ ç”Ÿæˆç•°å¸¸: {str(e)}')
                training_status['is_training'] = False

        # å•Ÿå‹•èƒŒæ™¯ç·šç¨‹
        thread = threading.Thread(target=run_generation, daemon=True)
        thread.start()

        # ç«‹å³è¿”å›æˆåŠŸï¼Œä¸ç­‰å¾…å®Œæˆ
        return jsonify({
            'success': True,
            'image_count': total_images,
            'message': f'å·²å•Ÿå‹•åœ–ç‰‡ç”Ÿæˆä»»å‹™ï¼Œé è¨ˆç”Ÿæˆ {total_images} å¼µè¨“ç·´åœ–ç‰‡',
            'status': 'generating',
            'estimated_time': len(stl_files) * 2,  # æ¯å€‹STLç´„2åˆ†é˜
            'log_file': log_file
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/validate_dataset', methods=['POST'])
def validate_dataset():
    """é©—è­‰è³‡æ–™é›†æ˜¯å¦å®Œæ•´"""
    try:
        data = request.get_json()
        stl_files = data.get('stl_files', [])

        missing_images = []
        incomplete = []
        complete = []

        for stl_file in stl_files:
            # ç§»é™¤ .stl å‰¯æª”å
            model_name = os.path.splitext(stl_file)[0] if stl_file.endswith('.stl') else stl_file
            image_dir = os.path.join('dataset', model_name)

            if not os.path.exists(image_dir):
                missing_images.append({
                    'name': model_name,
                    'status': 'missing',
                    'count': 0
                })
            else:
                # æª¢æŸ¥åœ–ç‰‡æ•¸é‡
                image_files = [f for f in os.listdir(image_dir) if f.endswith('.png')]
                image_count = len(image_files)

                if image_count < 360:
                    incomplete.append({
                        'name': model_name,
                        'status': 'incomplete',
                        'count': image_count,
                        'expected': 360
                    })
                else:
                    complete.append({
                        'name': model_name,
                        'status': 'complete',
                        'count': image_count
                    })

        is_valid = len(missing_images) == 0 and len(incomplete) == 0

        return jsonify({
            'success': True,
            'is_valid': is_valid,
            'complete': complete,
            'incomplete': incomplete,
            'missing': missing_images,
            'summary': {
                'total': len(stl_files),
                'complete': len(complete),
                'incomplete': len(incomplete),
                'missing': len(missing_images)
            }
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/get_class_names')
def get_class_names():
    """ç²å–åˆ†é¡åç¨±å°æ‡‰è¡¨"""
    try:
        # åˆä½µé è¨­å’Œè‡ªè¨‚åˆ†é¡åç¨±
        default_names = get_default_class_names()
        custom_names = load_class_names()

        # è‡ªè¨‚åç¨±å„ªå…ˆ
        for file_key, custom_name in custom_names.items():
            if file_key in default_names:
                default_names[file_key] = custom_name

        return jsonify({
            'success': True,
            'class_names': default_names
        })

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/update_class_names', methods=['POST'])
def update_class_names():
    """æ›´æ–°åˆ†é¡åç¨±"""
    try:
        data = request.get_json()
        class_names = data.get('class_names', {})

        if save_class_names(class_names):
            return jsonify({'success': True, 'message': 'åˆ†é¡åç¨±å·²æ›´æ–°'})
        else:
            return jsonify({'success': False, 'error': 'ä¿å­˜å¤±æ•—'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/reset_class_names', methods=['POST'])
def reset_class_names():
    """é‡ç½®ç‚ºé è¨­åˆ†é¡åç¨±"""
    try:
        # åˆªé™¤è‡ªè¨‚åˆ†é¡æª”æ¡ˆ
        if os.path.exists(class_names_file):
            os.remove(class_names_file)

        return jsonify({'success': True, 'message': 'å·²é‡ç½®ç‚ºé è¨­åˆ†é¡åç¨±'})

    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/version', methods=['GET'])
def get_version():
    """ç²å–ç³»çµ±ç‰ˆæœ¬è³‡è¨Š"""
    return jsonify({
        'version': APP_VERSION,
        'build_date': APP_BUILD_DATE,
        'faiss_available': FAISS_AVAILABLE
    })

# æ¨¡å‹åˆå§‹åŒ–æ¨™è¨˜
_model_init_attempted = False

# ç”¨æ–¼ Flask çš„ before_first_requestï¼ˆç¢ºä¿è‡³å°‘åŸ·è¡Œä¸€æ¬¡ï¼‰
@app.before_request
def initialize():
    """ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥"""
    global model_loaded, _model_init_attempted
    if not _model_init_attempted:
        _model_init_attempted = True
        print("ğŸ”„ é¦–æ¬¡è«‹æ±‚ï¼Œåˆå§‹åŒ–ç³»çµ±...")
        load_model()
        load_training_state()
        print("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ")

if __name__ == '__main__':
    # ç›´æ¥åŸ·è¡Œæ™‚çš„åˆå§‹åŒ–
    print("ğŸ”„ åˆå§‹åŒ–ç³»çµ±...")
    load_model()
    load_training_state()
    print("âœ… ç³»çµ±åˆå§‹åŒ–å®Œæˆ")

    # å•Ÿå‹•ç¶²é ä¼ºæœå™¨
    print("ğŸš€ å•Ÿå‹• FAISS è¾¨è­˜ç³»çµ±")
    print("=" * 50)
    print("ğŸ“± åŠŸèƒ½åŒ…å«:")
    print("  â€¢ æª”æ¡ˆä¸Šå‚³è­˜åˆ¥")
    print("  â€¢ ç›¸æ©Ÿå³æ™‚æ‹ç…§è­˜åˆ¥")
    print("  â€¢ æ•¸æ“šé›†æ¨£æœ¬æ¸¬è©¦")
    print("  â€¢ æ¨¡å‹è¨“ç·´å’Œç®¡ç†")
    print("  â€¢ STL æª”æ¡ˆè™•ç†")
    print("  â€¢ åˆ†é¡åç¨±è‡ªè¨‚")
    print("  â€¢ è¨“ç·´ç‹€æ…‹æŒä¹…åŒ–")
    print()
    print("ğŸŒ è«‹é–‹å•Ÿç€è¦½å™¨è¨ªå•: http://localhost:8082")
    print("âš ï¸  æŒ‰ Ctrl+C åœæ­¢ä¼ºæœå™¨")
    print()

def start_faiss_training(config):
    """å•Ÿå‹•FAISSå–®ç¨è¨“ç·´"""
    global training_process, training_status

    try:
        faiss_config = config.get('faiss_config', {})

        # æ§‹å»ºFAISSè¨“ç·´å‘½ä»¤ - ä½¿ç”¨ faiss_recognition.py
        # ä½¿ç”¨ -u åƒæ•¸è®“ Python ä½¿ç”¨ç„¡ç·©è¡è¼¸å‡º
        cmd = ['python3', '-u', 'faiss_recognition.py']

        training_status['log_lines'].append(f'ğŸ“ FAISS å‘½ä»¤: {" ".join(cmd)}')
        training_status['log_lines'].append(f'âš™ï¸ è¨“ç·´é…ç½®: Epochs={faiss_config.get("epochs", 100)}, Batch={faiss_config.get("batch_size", 16)}')
        training_status['log_lines'].append('â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”')
        training_status['log_lines'].append('ğŸ“Š è¨“ç·´æ—¥èªŒé–‹å§‹...')

        # å•Ÿå‹•FAISSè¨“ç·´é€²ç¨‹ - ä½¿ç”¨ unbuffered è¼¸å‡º
        env = os.environ.copy()
        env['PYTHONUNBUFFERED'] = '1'  # ç¢ºä¿ Python è¼¸å‡ºä¸è¢«ç·©è¡

        training_process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            universal_newlines=True,
            env=env
        )

        # å•Ÿå‹•ç›£æ§ç·šç¨‹
        monitor_thread = threading.Thread(target=monitor_training_process)
        monitor_thread.daemon = True
        monitor_thread.start()

        training_status['log_lines'].append('âœ… FAISS è¨“ç·´é€²ç¨‹å·²å•Ÿå‹•')
        training_status['log_lines'].append('â³ æ­£åœ¨ç­‰å¾…è¨“ç·´è¼¸å‡º...')

    except Exception as e:
        training_status['log_lines'].append(f'âŒ FAISS è¨“ç·´å•Ÿå‹•å¤±æ•—: {e}')
        training_status['is_training'] = False
        print(f"è¨“ç·´å•Ÿå‹•éŒ¯èª¤: {e}")

def start_faiss_training(config):
    """å•Ÿå‹•FAISSç´¢å¼•å»ºç«‹"""
    global training_status

    try:
        if not FAISS_AVAILABLE:
            training_status['log_lines'].append('âŒ FAISS ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥å®‰è£')
            training_status['is_training'] = False
            return

        training_status['log_lines'].append('ğŸ” é–‹å§‹å»ºç«‹ FAISS ç‰¹å¾µç´¢å¼•...')

        # åœ¨å–®ç¨ç·šç¨‹ä¸­å»ºç«‹FAISSç´¢å¼•
        faiss_thread = threading.Thread(target=build_faiss_index_async, args=(config,))
        faiss_thread.daemon = True
        faiss_thread.start()

    except Exception as e:
        training_status['log_lines'].append(f'âŒ FAISS ç´¢å¼•å»ºç«‹å¤±æ•—: {e}')
        training_status['is_training'] = False

def start_dual_training(config):
    """å•Ÿå‹•é›™æ¨¡å‹ä¸¦è¡Œè¨“ç·´"""
    global training_status

    try:
        training_status['log_lines'].append('ğŸ”„ åˆå§‹åŒ–ä¸¦è¡Œè¨“ç·´ç³»çµ±...')

        # å…ˆå•Ÿå‹•FAISSç´¢å¼•å»ºç«‹ï¼ˆé€šå¸¸è¼ƒå¿«ï¼‰
        training_status['log_lines'].append('ğŸ” æ­¥é©Ÿ 1/2: é–‹å§‹å»ºç«‹ FAISS ç‰¹å¾µç´¢å¼•...')
        if FAISS_AVAILABLE:
            faiss_thread = threading.Thread(target=build_faiss_index_async, args=(config,))
            faiss_thread.daemon = True
            faiss_thread.start()
        else:
            training_status['log_lines'].append('âš ï¸ FAISS ä¸å¯ç”¨ï¼Œè·³éç‰¹å¾µç´¢å¼•å»ºç«‹')

        # ç„¶å¾Œå•Ÿå‹•FAISSè¨“ç·´
        training_status['log_lines'].append('ğŸ¯ æ­¥é©Ÿ 2/2: é–‹å§‹ FAISS æ¨¡å‹è¨“ç·´...')
        start_faiss_training(config)

        training_status['log_lines'].append('âš¡ ä¸¦è¡Œè¨“ç·´ç³»çµ±å·²å•Ÿå‹•')

    except Exception as e:
        training_status['log_lines'].append(f'âŒ ä¸¦è¡Œè¨“ç·´å•Ÿå‹•å¤±æ•—: {e}')
        training_status['is_training'] = False

def build_faiss_index_async(config):
    """ç•°æ­¥å»ºç«‹FAISSç´¢å¼•"""
    global training_status

    try:
        from faiss_recognition import initialize_faiss

        training_status['log_lines'].append('ğŸ“¦ è¼‰å…¥ FAISS è­˜åˆ¥å¼•æ“...')

        # åˆå§‹åŒ–FAISSå¼•æ“
        if initialize_faiss():
            training_status['log_lines'].append('âœ… FAISS ç‰¹å¾µç´¢å¼•å»ºç«‹å®Œæˆ')
        else:
            training_status['log_lines'].append('âŒ FAISS ç‰¹å¾µç´¢å¼•å»ºç«‹å¤±æ•—')

        # å¦‚æœåªé€²è¡ŒFAISSè¨“ç·´ï¼Œå‰‡å®Œæˆå¾ŒçµæŸ
        if training_status.get('faiss_enabled') and not training_status.get('faiss_enabled'):
            training_status['is_training'] = False
            training_status['log_lines'].append('ğŸ‰ FAISS è¨“ç·´å®Œæˆ')

    except Exception as e:
        training_status['log_lines'].append(f'âŒ FAISS ç•°æ­¥å»ºç«‹å¤±æ•—: {e}')

# ==================== ç³»çµ±è¨­å®šç›¸é—œ API ====================

@app.route('/settings')
def settings_page():
    """ç³»çµ±è¨­å®šé é¢"""
    return render_template('settings.html')

@app.route('/api/get_models')
def get_models():
    """ç²å–æ¨¡å‹åˆ—è¡¨ï¼ˆå…¼å®¹èˆŠç‰ˆ APIï¼‰- FAISS ç‰ˆæœ¬"""
    try:
        import pickle

        # æª¢æŸ¥ FAISS æ¨¡å‹æª”æ¡ˆ
        faiss_index_path = Path('faiss_features.index')
        faiss_labels_path = Path('faiss_labels.pkl')

        current_model = None
        available_models = []

        if faiss_index_path.exists() and faiss_labels_path.exists():
            # ç²å–æª”æ¡ˆä¿¡æ¯
            index_stat = faiss_index_path.stat()
            labels_stat = faiss_labels_path.stat()
            total_size = index_stat.st_size + labels_stat.st_size
            size_mb = total_size / (1024 * 1024)

            # è®€å–æ¨™ç±¤ç²å–é¡åˆ¥ä¿¡æ¯
            try:
                with open(faiss_labels_path, 'rb') as f:
                    labels_data = pickle.load(f)
                    if isinstance(labels_data, dict):
                        class_names = labels_data.get('classes', [])
                        label_array = labels_data.get('labels', [])
                        num_classes = len(class_names)
                        num_features = len(label_array)
                    else:
                        num_classes = len(set(labels_data))
                        num_features = len(labels_data)
                        class_names = sorted(set(labels_data))
            except:
                num_classes = 0
                num_features = 0
                class_names = []

            # æ§‹å»ºç•¶å‰ä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
            current_model = {
                'name': 'FAISS è­˜åˆ¥æ¨¡å‹',
                'path': str(faiss_index_path),
                'size': f'{size_mb:.2f} MB',
                'classes': f'{num_classes} å€‹é¡åˆ¥',
                'created_date': datetime.fromtimestamp(index_stat.st_mtime).strftime('%Y-%m-%d %H:%M'),
                'status': 'ä½¿ç”¨ä¸­' if FAISS_AVAILABLE else 'æœªè¼‰å…¥',
                'accuracy': None,  # å¯ä»¥å¾é©—è­‰çµæœç²å–
                'num_features': num_features,
                'class_names': class_names
            }

        return jsonify({
            'success': True,
            'current_model': current_model,
            'available_models': available_models,
            'models': available_models
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/save_settings', methods=['POST'])
def save_settings():
    """å„²å­˜ç³»çµ±è¨­å®š"""
    try:
        settings = request.json
        import sqlite3

        db_path = 'system_data.db'
        with sqlite3.connect(db_path) as conn:
            cursor = conn.cursor()

            # å»ºç«‹æˆ–æ›´æ–°è¨­å®šè¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS system_settings (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # å„²å­˜æ¯å€‹è¨­å®š
            for key, value in settings.items():
                cursor.execute('''
                    INSERT OR REPLACE INTO system_settings (key, value, updated_at)
                    VALUES (?, ?, CURRENT_TIMESTAMP)
                ''', (key, str(value)))

            conn.commit()

        return jsonify({'success': True, 'message': 'è¨­å®šå·²å„²å­˜'})
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/get_settings')
def get_settings():
    """ç²å–ç³»çµ±è¨­å®š"""
    try:
        import sqlite3

        db_path = 'system_data.db'
        with sqlite3.connect(db_path) as conn:
            cursor = conn.cursor()

            # ç¢ºä¿è¡¨å­˜åœ¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS system_settings (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # ç²å–æ‰€æœ‰è¨­å®š
            cursor.execute('SELECT key, value FROM system_settings')
            rows = cursor.fetchall()

            settings = {}
            for key, value in rows:
                # å˜—è©¦è½‰æ›ç‚ºæ•¸å€¼
                try:
                    if '.' in value:
                        settings[key] = float(value)
                    else:
                        settings[key] = int(value)
                except:
                    settings[key] = value

            # æä¾›é è¨­å€¼
            default_settings = {
                'confidence_threshold': 0.25,
                'nms_threshold': 0.45,
                'max_detections': 10,
                'input_size': 640
            }

            for key, default_value in default_settings.items():
                if key not in settings:
                    settings[key] = default_value

        return jsonify({'success': True, 'settings': settings})
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/system_info')
def system_info():
    """ç²å–ç³»çµ±è³‡è¨Š"""
    try:
        import platform
        import subprocess

        # CPU ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=0.1)

        # è¨˜æ†¶é«”è³‡è¨Š
        memory = psutil.virtual_memory()
        memory_percent = memory.percent

        # GPU è³‡è¨Š
        gpu_memory = 'N/A'
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=memory.used,memory.total', '--format=csv,noheader,nounits'],
                capture_output=True,
                text=True,
                timeout=2
            )
            if result.returncode == 0:
                gpu_data = result.stdout.strip().split(', ')
                if len(gpu_data) >= 2:
                    used = float(gpu_data[0])
                    total = float(gpu_data[1])
                    gpu_memory = f"{used:.0f}/{total:.0f} MB"
        except:
            pass

        info = {
            'success': True,
            'python_version': platform.python_version(),
            'system': platform.system(),
            'machine': platform.machine(),
            'processor': platform.processor(),
            'cpu_count': psutil.cpu_count(),
            'cpu_percent': cpu_percent,
            'memory_percent': memory_percent,
            'memory_total': f"{memory.total / (1024**3):.2f} GB",
            'disk_total': f"{psutil.disk_usage('/').total / (1024**3):.2f} GB",
            'gpu_memory': gpu_memory
        }
        return jsonify(info)
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/list_stl_files')
def list_stl_files():
    """åˆ—å‡º STL è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰ STL æª”æ¡ˆ"""
    try:
        import os
        from pathlib import Path

        stl_dir = Path('STL')
        if not stl_dir.exists():
            return jsonify({'success': True, 'files': [], 'total': 0})

        stl_files = []
        for file_path in stl_dir.glob('*.stl'):
            stat = file_path.stat()

            # æª¢æŸ¥å°æ‡‰çš„è³‡æ–™é›†åœ–ç‰‡æ•¸é‡
            model_name = file_path.stem  # ä¸å«å‰¯æª”åçš„æª”æ¡ˆå
            dataset_dir = Path('dataset') / model_name
            image_count = 0
            if dataset_dir.exists():
                image_count = len(list(dataset_dir.glob('*.png')))

            stl_files.append({
                'name': file_path.name,
                'path': str(file_path),
                'size': stat.st_size,
                'size_mb': round(stat.st_size / (1024 * 1024), 2),
                'modified_time': datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S'),
                'created_time': datetime.fromtimestamp(stat.st_ctime).strftime('%Y-%m-%d %H:%M:%S'),
                'image_count': image_count
            })

        # æŒ‰åç¨±æ’åº
        stl_files.sort(key=lambda x: x['name'])

        return jsonify({
            'success': True,
            'files': stl_files,
            'total': len(stl_files)
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/statistics')
def get_statistics():
    """ç²å–ç³»çµ±çµ±è¨ˆè³‡æ–™"""
    try:
        stats = data_manager.get_statistics()
        # ç›´æ¥è¿”å›çµ±è¨ˆæ•¸æ“šåœ¨é ‚å±¤ï¼Œè€Œä¸æ˜¯åµŒå¥—åœ¨ statistics ä¸‹
        return jsonify({
            'success': True,
            'stl_count': stats.get('stl_count', 0),
            'image_count': stats.get('image_count', 0),
            'recognition_count': stats.get('recognition_count', 0),
            'class_count': stats.get('class_count', 0)
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/upload_history')
def get_upload_history():
    """ç²å–ä¸Šå‚³æ­·å²"""
    try:
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 20))
        offset = (page - 1) * limit

        records = data_manager.get_upload_history(limit=limit, offset=offset)
        return jsonify({'success': True, 'records': records, 'page': page})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/recognition_history')
def get_recognition_history():
    """ç²å–è¾¨è­˜æ­·å²"""
    try:
        import sqlite3
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 20))
        offset = (page - 1) * limit

        # ç›´æ¥æŸ¥è©¢è³‡æ–™åº«
        db_path = 'system_data.db'
        with sqlite3.connect(db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()

            # ç²å–è¨˜éŒ„ï¼ŒJOIN upload_history ä¾†ç²å–åŸå§‹ä¸Šå‚³ç…§ç‰‡
            cursor.execute('''
                SELECT
                    r.*,
                    u.file_path as upload_file_path,
                    u.filename as upload_filename
                FROM recognition_history r
                LEFT JOIN upload_history u ON r.upload_id = u.id
                ORDER BY r.timestamp DESC
                LIMIT ? OFFSET ?
            ''', (limit, offset))
            rows = cursor.fetchall()
            records = [dict(row) for row in rows]

            # è½‰æ›åœ–ç‰‡è·¯å¾‘ç‚ºæ­£ç¢ºçš„ URLï¼Œä¸¦æ·»åŠ  STL åƒè€ƒåœ–ç‰‡
            for record in records:
                # è™•ç†ä¸Šå‚³ç…§ç‰‡è·¯å¾‘
                if record.get('upload_file_path'):
                    # ç§»é™¤ 'web_uploads/' å‰ç¶´ï¼Œç„¶å¾ŒåŠ ä¸Š '/web_uploads/'
                    path = record['upload_file_path']
                    if path.startswith('web_uploads/'):
                        path = path[len('web_uploads/'):]
                    if not path.startswith('/'):
                        path = '/' + path
                    if not path.startswith('/web_uploads/'):
                        record['upload_file_path'] = f"/web_uploads{path}"
                    else:
                        record['upload_file_path'] = path

                # æ·»åŠ  STL åƒè€ƒåœ–ç‰‡è·¯å¾‘ - æ‰¾å‡ºæœ€ç›¸ä¼¼çš„åœ–ç‰‡
                if record.get('predicted_class') and record.get('upload_file_path'):
                    import os
                    import cv2
                    import numpy as np

                    class_name = record['predicted_class']
                    dataset_path = f'dataset/{class_name}'

                    # ç§»é™¤è·¯å¾‘å‰ç¶´ä»¥ç²å–å¯¦éš›æª”æ¡ˆè·¯å¾‘
                    upload_path = record['upload_file_path']
                    if upload_path.startswith('/web_uploads/'):
                        upload_path = 'web_uploads/' + upload_path[len('/web_uploads/'):]

                    # æª¢æŸ¥è³‡æ–™å¤¾æ˜¯å¦å­˜åœ¨
                    if os.path.exists(dataset_path) and os.path.exists(upload_path):
                        try:
                            # è®€å–ä¸Šå‚³çš„åœ–ç‰‡
                            upload_img = cv2.imread(upload_path)
                            if upload_img is None:
                                record['stl_reference_image'] = None
                                continue

                            # èª¿æ•´å¤§å°ä»¥åŠ å¿«æ¯”å°é€Ÿåº¦
                            upload_img = cv2.resize(upload_img, (256, 256))
                            upload_gray = cv2.cvtColor(upload_img, cv2.COLOR_BGR2GRAY)

                            # ç²å–è©²é¡åˆ¥çš„æ‰€æœ‰åœ–ç‰‡
                            images = [f for f in os.listdir(dataset_path) if f.endswith('.png')]

                            if images:
                                # å„²å­˜æ‰€æœ‰åŒ¹é…çµæœçš„åˆ—è¡¨ (åœ–ç‰‡åç¨±, ç›¸ä¼¼åº¦åˆ†æ•¸)
                                matches = []

                                # åªæ¯”å°å‰30å¼µåœ–ç‰‡ä»¥ç¯€çœæ™‚é–“ï¼ˆæˆ–å…¨éƒ¨æ¯”å°å¦‚æœå°‘æ–¼30å¼µï¼‰
                                sample_images = images[:min(30, len(images))]

                                for img_name in sample_images:
                                    img_path = os.path.join(dataset_path, img_name)
                                    ref_img = cv2.imread(img_path)

                                    if ref_img is not None:
                                        # èª¿æ•´å¤§å°
                                        ref_img = cv2.resize(ref_img, (256, 256))
                                        ref_gray = cv2.cvtColor(ref_img, cv2.COLOR_BGR2GRAY)

                                        # ä½¿ç”¨çµæ§‹ç›¸ä¼¼æ€§æŒ‡æ•¸ï¼ˆSSIMï¼‰æˆ–ç°¡å–®çš„ç›´æ–¹åœ–æ¯”å°
                                        # é€™è£¡ä½¿ç”¨ç›´æ–¹åœ–æ¯”å°ï¼Œæ›´å¿«é€Ÿ
                                        upload_hist = cv2.calcHist([upload_gray], [0], None, [256], [0, 256])
                                        ref_hist = cv2.calcHist([ref_gray], [0], None, [256], [0, 256])

                                        # æ­¸ä¸€åŒ–
                                        upload_hist = cv2.normalize(upload_hist, upload_hist).flatten()
                                        ref_hist = cv2.normalize(ref_hist, ref_hist).flatten()

                                        # è¨ˆç®—ç›¸é—œæ€§
                                        score = cv2.compareHist(upload_hist, ref_hist, cv2.HISTCMP_CORREL)

                                        # å°‡ç›¸é—œæ€§åˆ†æ•¸è½‰æ›ç‚ºç™¾åˆ†æ¯” (ç¯„åœ -1 åˆ° 1ï¼Œè½‰ç‚º 0 åˆ° 100)
                                        similarity_percent = max(0, min(100, (score + 1) * 50))

                                        matches.append({
                                            'image': img_name,
                                            'score': score,
                                            'similarity_percent': round(similarity_percent, 2)
                                        })

                                # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•¸æ’åºï¼Œå–å‰ä¸‰å
                                matches.sort(key=lambda x: x['score'], reverse=True)
                                top_matches = matches[:3]

                                if top_matches:
                                    # å„²å­˜å‰ä¸‰å¼µæœ€ç›¸ä¼¼çš„åœ–ç‰‡
                                    record['stl_reference_images'] = [
                                        {
                                            'path': f'/dataset/{class_name}/{match["image"]}',
                                            'similarity': match['similarity_percent']
                                        }
                                        for match in top_matches
                                    ]
                                    # ä¿ç•™æœ€é«˜ç›¸ä¼¼åº¦ä½œç‚ºä¸»è¦åˆ†æ•¸
                                    record['similarity_score'] = top_matches[0]['similarity_percent']
                                    # å‘ä¸‹å…¼å®¹ï¼šç¬¬ä¸€å¼µåœ–ç‰‡ä½œç‚ºä¸»è¦åƒè€ƒåœ–
                                    record['stl_reference_image'] = record['stl_reference_images'][0]['path']
                                else:
                                    record['stl_reference_images'] = []
                                    record['stl_reference_image'] = None
                                    record['similarity_score'] = 0
                            else:
                                record['stl_reference_image'] = None
                                record['similarity_score'] = 0
                        except Exception as e:
                            print(f"åœ–ç‰‡æ¯”å°éŒ¯èª¤: {e}")
                            # å¦‚æœæ¯”å°å¤±æ•—ï¼Œä½¿ç”¨ç¬¬ä¸€å¼µåœ–ç‰‡ä½œç‚ºå¾Œå‚™
                            images = [f for f in os.listdir(dataset_path) if f.endswith('.png')]
                            if images:
                                record['stl_reference_image'] = f'/dataset/{class_name}/{images[0]}'
                            else:
                                record['stl_reference_image'] = None
                            record['similarity_score'] = 0
                    else:
                        record['stl_reference_image'] = None
                        record['similarity_score'] = 0
                else:
                    record['stl_reference_image'] = None
                    record['similarity_score'] = 0

            # ç²å–ç¸½æ•¸
            cursor.execute('SELECT COUNT(*) FROM recognition_history')
            total = cursor.fetchone()[0]

        return jsonify({'success': True, 'data': records, 'records': records, 'total': total, 'page': page})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/training_history')
def get_training_history_api():
    """ç²å–è¨“ç·´æ­·å²"""
    try:
        page = int(request.args.get('page', 1))
        limit = int(request.args.get('limit', 50))
        offset = (page - 1) * limit

        records = data_manager.get_training_history(limit=limit, offset=offset)
        return jsonify({'success': True, 'records': records, 'page': page})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/models_detailed')
def get_models_detailed():
    """ç²å–è©³ç´°çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        models = model_manager.scan_models()
        summary = model_manager.get_model_summary()
        return jsonify({
            'success': True,
            'models': models,
            'summary': summary
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/activate_model', methods=['POST'])
def activate_model():
    """å•Ÿç”¨æŒ‡å®šæ¨¡å‹"""
    try:
        data = request.json
        model_path = data.get('model_path')

        if model_manager.load_model(model_path):
            # æ›´æ–°å…¨åŸŸæ¨¡å‹è®Šæ•¸
            global model, model_loaded, model_info
            model = model_manager.get_current_model()
            model_loaded = True
            model_info['path'] = model_path

            # è¨»å†Šåˆ°è³‡æ–™ç®¡ç†å™¨
            data_manager.set_active_model(model_path)

            return jsonify({'success': True, 'message': 'æ¨¡å‹å·²å•Ÿç”¨'})
        else:
            return jsonify({'success': False, 'error': 'æ¨¡å‹è¼‰å…¥å¤±æ•—'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/backup_model', methods=['POST'])
def backup_model_api():
    """å‚™ä»½æ¨¡å‹"""
    try:
        data = request.json
        model_path = data.get('model_path')

        if model_manager.backup_model(model_path):
            return jsonify({'success': True, 'message': 'æ¨¡å‹å·²å‚™ä»½'})
        else:
            return jsonify({'success': False, 'error': 'å‚™ä»½å¤±æ•—'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/export_data')
def export_data_api():
    """åŒ¯å‡ºç³»çµ±è³‡æ–™"""
    try:
        export_type = request.args.get('type', 'all')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = f'export_{export_type}_{timestamp}.json'

        table_map = {
            'upload': 'upload_history',
            'recognition': 'recognition_history',
            'training': 'training_history',
            'all': None
        }

        table_name = table_map.get(export_type)
        data_manager.export_data(output_path, table_name)

        return send_file(output_path, as_attachment=True)
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

@app.route('/api/clear_old_data', methods=['POST'])
def clear_old_data_api():
    """æ¸…ç†èˆŠè³‡æ–™"""
    try:
        days = int(request.json.get('days', 30))
        data_manager.clear_old_records(days=days)
        return jsonify({'success': True, 'message': f'å·²æ¸…ç† {days} å¤©å‰çš„è³‡æ–™'})
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

# å®Œæ•´é‡æ–°è¨“ç·´ç›¸é—œ API
@app.route('/api/delete_dataset', methods=['POST'])
def delete_dataset_api():
    """åˆªé™¤æ‰€æœ‰è³‡æ–™é›†"""
    try:
        import shutil
        dataset_path = 'dataset'
        deleted_folders = 0
        deleted_images = 0

        if os.path.exists(dataset_path):
            # çµ±è¨ˆè³‡è¨Š
            for folder in os.listdir(dataset_path):
                folder_path = os.path.join(dataset_path, folder)
                if os.path.isdir(folder_path):
                    deleted_folders += 1
                    images = [f for f in os.listdir(folder_path) if f.endswith('.png')]
                    deleted_images += len(images)

            # åˆªé™¤æ•´å€‹è³‡æ–™å¤¾
            shutil.rmtree(dataset_path)

            # é‡æ–°å»ºç«‹ç©ºè³‡æ–™å¤¾
            os.makedirs(dataset_path, exist_ok=True)

        return jsonify({
            'success': True,
            'deleted_folders': deleted_folders,
            'deleted_images': deleted_images
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)})

# åœ–ç‰‡ç”Ÿæˆç‹€æ…‹ï¼ˆç”¨æ–¼å‰ç«¯ç›£æ§ï¼‰
image_generation_status = {
    'is_generating': False,
    'progress': 0,
    'current_model': 0,
    'total_models': 0,
    'current_model_name': '',
    'log_lines': [],
    'success': False,
    'error': None,
    'total_images': 0
}

@app.route('/api/generate_all_images', methods=['POST'])
def generate_all_images_api():
    """ç”Ÿæˆæ‰€æœ‰åœ–ç‰‡"""
    global image_generation_status

    try:
        # ç²å–æ¨¡å¼åƒæ•¸
        data = request.get_json() or {}
        mode = data.get('mode', 'normal')  # 'normal' æˆ– 'precision'

        # é‡ç½®ç‹€æ…‹
        image_generation_status = {
            'is_generating': True,
            'progress': 0,
            'current_model': 0,
            'total_models': 0,
            'current_model_name': '',
            'log_lines': [],
            'success': False,
            'error': None,
            'total_images': 0,
            'mode': mode
        }

        # æƒæ STL æª”æ¡ˆ
        stl_files = glob.glob('STL/*.stl')
        image_generation_status['total_models'] = len(stl_files)

        # åœ¨èƒŒæ™¯åŸ·è¡Œåœ–ç‰‡ç”Ÿæˆ
        import threading
        thread = threading.Thread(target=generate_images_thread, args=(stl_files, mode))
        thread.daemon = True
        thread.start()

        return jsonify({
            'success': True,
            'stl_count': len(stl_files),
            'mode': mode
        })
    except Exception as e:
        image_generation_status['is_generating'] = False
        image_generation_status['error'] = str(e)
        return jsonify({'success': False, 'error': str(e)})

def generate_images_thread(stl_files, mode='normal'):
    """èƒŒæ™¯åŸ·è¡Œç·’ï¼šç”Ÿæˆåœ–ç‰‡"""
    global image_generation_status

    try:
        total = len(stl_files)
        images_per_model = 60 if mode == 'normal' else 360

        mode_name = 'æ­£å¸¸è¨“ç·´' if mode == 'normal' else 'ç²¾æº–è¨“ç·´'
        image_generation_status['log_lines'].append(f'ğŸ“¸ é–‹å§‹ç”Ÿæˆ {total} å€‹æ¨¡å‹çš„åœ–ç‰‡ ({mode_name})')
        image_generation_status['log_lines'].append(f'ğŸ“Š æ¯å€‹æ¨¡å‹å°‡ç”Ÿæˆ {images_per_model} å¼µåœ–ç‰‡')

        if mode == 'precision':
            image_generation_status['log_lines'].append(f'âœ¨ ç²¾æº–è¨“ç·´æ¨¡å¼ï¼šåŒ…å«å…‰ç…§å¹²æ“¾å’ŒèƒŒæ™¯å¹²æ“¾')

        estimated_time = total * (1 if mode == 'normal' else 3)
        image_generation_status['log_lines'].append(f'â±ï¸ é è¨ˆè€—æ™‚: {estimated_time} åˆ†é˜')

        # æ ¹æ“šæ¨¡å¼é¸æ“‡ä¸åŒçš„ç”Ÿæˆè…³æœ¬
        if mode == 'normal':
            # æ­£å¸¸è¨“ç·´ï¼š60å¼µï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬
            result = subprocess.run(
                ['python', 'generate_images_normal.py'],
                capture_output=True,
                text=True,
                timeout=1800  # 30åˆ†é˜è¶…æ™‚
            )
        else:
            # ç²¾æº–è¨“ç·´ï¼š360å¼µï¼Œä½¿ç”¨å®Œæ•´ç‰ˆæœ¬ï¼ˆå«å¹²æ“¾ï¼‰
            result = subprocess.run(
                ['python', 'generate_images_color.py'],
                capture_output=True,
                text=True,
                timeout=3600  # 60åˆ†é˜è¶…æ™‚
            )

        if result.returncode == 0:
            # çµ±è¨ˆç”Ÿæˆçš„åœ–ç‰‡
            total_images = 0
            for idx, stl_path in enumerate(stl_files):
                model_name = os.path.splitext(os.path.basename(stl_path))[0]

                # æ›´æ–°é€²åº¦
                progress = int(((idx + 1) / total) * 100)
                image_generation_status['current_model'] = idx + 1
                image_generation_status['current_model_name'] = model_name
                image_generation_status['progress'] = progress

                dataset_folder = os.path.join('dataset', model_name)
                if os.path.exists(dataset_folder):
                    images = [f for f in os.listdir(dataset_folder) if f.endswith('.png')]
                    img_count = len(images)
                    total_images += img_count
                    image_generation_status['log_lines'].append(f'âœ… {model_name}: {img_count} å¼µåœ–ç‰‡')

            # å®Œæˆ
            image_generation_status['is_generating'] = False
            image_generation_status['progress'] = 100
            image_generation_status['success'] = True
            image_generation_status['total_images'] = total_images
            image_generation_status['log_lines'].append(f'ğŸ‰ åœ–ç‰‡ç”Ÿæˆå®Œæˆï¼å…± {total_images} å¼µ')
        else:
            raise Exception(result.stderr or 'ç”Ÿæˆå¤±æ•—')

    except Exception as e:
        image_generation_status['is_generating'] = False
        image_generation_status['success'] = False
        image_generation_status['error'] = str(e)
        image_generation_status['log_lines'].append(f'âŒ éŒ¯èª¤: {str(e)}')

@app.route('/api/image_generation_status')
def image_generation_status_api():
    """æŸ¥è©¢åœ–ç‰‡ç”Ÿæˆç‹€æ…‹"""
    return jsonify(image_generation_status)

@app.route('/api/generate_images', methods=['POST'])
def generate_single_stl_images():
    """ç‚ºå–®å€‹ STL æª”æ¡ˆç”Ÿæˆåœ–ç‰‡"""
    try:
        data = request.get_json()
        stl_file = data.get('stl_file')

        if not stl_file:
            return jsonify({'success': False, 'error': 'æœªæä¾› STL æª”æ¡ˆåç¨±'}), 400

        stl_path = os.path.join('STL', stl_file)

        if not os.path.exists(stl_path):
            return jsonify({'success': False, 'error': 'STL æª”æ¡ˆä¸å­˜åœ¨'}), 404

        # åœ¨èƒŒæ™¯åŸ·è¡Œåœ–ç‰‡ç”Ÿæˆ
        import threading
        thread = threading.Thread(target=generate_single_images_thread, args=(stl_file,))
        thread.daemon = True
        thread.start()

        return jsonify({
            'success': True,
            'message': f'é–‹å§‹ç‚º {stl_file} ç”Ÿæˆåœ–ç‰‡'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

def generate_single_images_thread(stl_file):
    """èƒŒæ™¯åŸ·è¡Œç·’ï¼šç‚ºå–®å€‹ STL ç”Ÿæˆåœ–ç‰‡"""
    try:
        # ä½¿ç”¨ generate_images_color.pyï¼Œåªè™•ç†æŒ‡å®šçš„ STL
        model_name = os.path.splitext(stl_file)[0]

        # å‰µå»ºè‡¨æ™‚ Python è…³æœ¬ä¾†ç”Ÿæˆå–®å€‹æ¨¡å‹çš„åœ–ç‰‡
        script_content = f'''
import sys
sys.path.insert(0, '.')
from generate_images_color import generate_images_for_model

# åªç”ŸæˆæŒ‡å®šæ¨¡å‹çš„åœ–ç‰‡
stl_path = "STL/{stl_file}"
generate_images_for_model(stl_path, "{model_name}")
print(f"âœ… {{model_name}} åœ–ç‰‡ç”Ÿæˆå®Œæˆ")
'''

        # å¯«å…¥è‡¨æ™‚è…³æœ¬
        temp_script = f'temp_generate_{model_name}.py'
        with open(temp_script, 'w', encoding='utf-8') as f:
            f.write(script_content)

        # åŸ·è¡Œç”Ÿæˆ
        result = subprocess.run(
            ['python', temp_script],
            capture_output=True,
            text=True,
            timeout=600  # 10åˆ†é˜è¶…æ™‚
        )

        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        if os.path.exists(temp_script):
            os.remove(temp_script)

    except Exception as e:
        logger.error(f"ç”Ÿæˆåœ–ç‰‡å¤±æ•—: {e}")

@app.route('/api/delete_stl', methods=['POST'])
def delete_stl_file():
    """åˆªé™¤ STL æª”æ¡ˆåŠå…¶å°æ‡‰çš„è³‡æ–™é›†"""
    try:
        data = request.get_json()
        filename = data.get('filename')

        if not filename:
            return jsonify({'success': False, 'error': 'æœªæä¾›æª”æ¡ˆåç¨±'}), 400

        stl_path = os.path.join('STL', filename)

        if not os.path.exists(stl_path):
            return jsonify({'success': False, 'error': 'STL æª”æ¡ˆä¸å­˜åœ¨'}), 404

        # åˆªé™¤ STL æª”æ¡ˆ
        os.remove(stl_path)

        # åˆªé™¤å°æ‡‰çš„è³‡æ–™é›†è³‡æ–™å¤¾
        model_name = os.path.splitext(filename)[0]
        dataset_path = os.path.join('dataset', model_name)

        if os.path.exists(dataset_path):
            import shutil
            shutil.rmtree(dataset_path)

        return jsonify({
            'success': True,
            'message': f'å·²åˆªé™¤ {filename} åŠå…¶è³‡æ–™é›†'
        })
    except Exception as e:
        return jsonify({'success': False, 'error': str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=8082)