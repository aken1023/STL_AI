"""
CLIP Feature Extractor for STL Image Recognition
ä½¿ç”¨ CLIP æ¨¡å‹é€²è¡Œå¤šæ¨¡æ…‹ç‰¹å¾µæå–

æ”¯æ´åŠŸèƒ½ï¼š
- åœ–åƒç‰¹å¾µæå–ï¼ˆCLIP Vision Encoderï¼‰
- æ–‡å­—ç‰¹å¾µæå–ï¼ˆCLIP Text Encoderï¼‰
- æ‰¹æ¬¡è™•ç†
- GPU åŠ é€Ÿ
"""

import os
import torch
import clip
import numpy as np
from PIL import Image
from pathlib import Path
from typing import List, Union, Tuple
import pickle
import logging

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CLIPFeatureExtractor:
    """CLIP ç‰¹å¾µæå–å™¨"""

    def __init__(self, model_name: str = "ViT-B/32", device: str = None):
        """
        åˆå§‹åŒ– CLIP æ¨¡å‹

        Args:
            model_name: CLIP æ¨¡å‹åç¨±
                - ViT-B/32: åŸºç¤ç‰ˆæœ¬ï¼Œé€Ÿåº¦å¿«ï¼ˆ512 ç¶­ï¼‰
                - ViT-B/16: ä¸­ç­‰ç‰ˆæœ¬ï¼Œå¹³è¡¡æ€§èƒ½ï¼ˆ512 ç¶­ï¼‰
                - ViT-L/14: å¤§å‹ç‰ˆæœ¬ï¼Œæº–ç¢ºåº¦é«˜ï¼ˆ768 ç¶­ï¼‰
            device: é‹ç®—è£ç½® ('cuda' æˆ– 'cpu')
        """
        # è‡ªå‹•é¸æ“‡è£ç½®
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device

        logger.info(f"ğŸš€ åˆå§‹åŒ– CLIP æ¨¡å‹: {model_name}")
        logger.info(f"ğŸ’» ä½¿ç”¨è£ç½®: {self.device}")

        # è¼‰å…¥ CLIP æ¨¡å‹
        self.model, self.preprocess = clip.load(model_name, device=self.device)
        self.model.eval()  # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼

        # å–å¾—ç‰¹å¾µç¶­åº¦
        self.feature_dim = self.model.visual.output_dim
        logger.info(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼ç‰¹å¾µç¶­åº¦: {self.feature_dim}")

    def extract_image_features(self, image_path: Union[str, Path]) -> np.ndarray:
        """
        å¾å–®å¼µåœ–ç‰‡æå– CLIP ç‰¹å¾µ

        Args:
            image_path: åœ–ç‰‡è·¯å¾‘

        Returns:
            ç‰¹å¾µå‘é‡ (numpy array)
        """
        try:
            # è¼‰å…¥ä¸¦é è™•ç†åœ–ç‰‡
            image = Image.open(image_path).convert('RGB')
            image_input = self.preprocess(image).unsqueeze(0).to(self.device)

            # æå–ç‰¹å¾µ
            with torch.no_grad():
                features = self.model.encode_image(image_input)
                # L2 æ­£è¦åŒ–
                features = features / features.norm(dim=-1, keepdim=True)

            return features.cpu().numpy().flatten()

        except Exception as e:
            logger.error(f"âŒ æå–åœ–ç‰‡ç‰¹å¾µå¤±æ•— {image_path}: {e}")
            return None

    def extract_text_features(self, text: str) -> np.ndarray:
        """
        å¾æ–‡å­—æè¿°æå– CLIP ç‰¹å¾µ

        Args:
            text: æ–‡å­—æè¿°

        Returns:
            ç‰¹å¾µå‘é‡ (numpy array)
        """
        try:
            # å°‡æ–‡å­—è½‰æ›ç‚º token
            text_input = clip.tokenize([text]).to(self.device)

            # æå–ç‰¹å¾µ
            with torch.no_grad():
                features = self.model.encode_text(text_input)
                # L2 æ­£è¦åŒ–
                features = features / features.norm(dim=-1, keepdim=True)

            return features.cpu().numpy().flatten()

        except Exception as e:
            logger.error(f"âŒ æå–æ–‡å­—ç‰¹å¾µå¤±æ•—: {e}")
            return None

    def extract_batch_image_features(self, image_paths: List[Union[str, Path]],
                                     batch_size: int = 32) -> Tuple[np.ndarray, List[str]]:
        """
        æ‰¹æ¬¡æå–åœ–ç‰‡ç‰¹å¾µ

        Args:
            image_paths: åœ–ç‰‡è·¯å¾‘åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            (ç‰¹å¾µçŸ©é™£, æˆåŠŸè™•ç†çš„åœ–ç‰‡è·¯å¾‘åˆ—è¡¨)
        """
        features_list = []
        valid_paths = []

        total = len(image_paths)
        logger.info(f"ğŸ“Š é–‹å§‹æ‰¹æ¬¡è™•ç† {total} å¼µåœ–ç‰‡...")

        for i in range(0, total, batch_size):
            batch_paths = image_paths[i:i + batch_size]
            batch_images = []
            batch_valid_paths = []

            # è¼‰å…¥æ‰¹æ¬¡åœ–ç‰‡
            for path in batch_paths:
                try:
                    image = Image.open(path).convert('RGB')
                    batch_images.append(self.preprocess(image))
                    batch_valid_paths.append(str(path))
                except Exception as e:
                    logger.warning(f"âš ï¸ è·³éæå£çš„åœ–ç‰‡ {path}: {e}")
                    continue

            if len(batch_images) == 0:
                continue

            # æ‰¹æ¬¡è™•ç†
            try:
                batch_input = torch.stack(batch_images).to(self.device)

                with torch.no_grad():
                    batch_features = self.model.encode_image(batch_input)
                    # L2 æ­£è¦åŒ–
                    batch_features = batch_features / batch_features.norm(dim=-1, keepdim=True)

                features_list.append(batch_features.cpu().numpy())
                valid_paths.extend(batch_valid_paths)

                # é¡¯ç¤ºé€²åº¦
                processed = min(i + batch_size, total)
                logger.info(f"â³ é€²åº¦: {processed}/{total} ({processed/total*100:.1f}%)")

            except Exception as e:
                logger.error(f"âŒ æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
                continue

        if len(features_list) == 0:
            logger.error("âŒ æ²’æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾µ")
            return None, []

        # åˆä½µæ‰€æœ‰æ‰¹æ¬¡
        all_features = np.vstack(features_list)
        logger.info(f"âœ… æˆåŠŸæå– {len(valid_paths)} å¼µåœ–ç‰‡çš„ç‰¹å¾µ")

        return all_features, valid_paths

    def build_dataset_index(self, dataset_dir: Union[str, Path],
                           output_dir: Union[str, Path] = ".",
                           batch_size: int = 32) -> bool:
        """
        ç‚ºæ•´å€‹è³‡æ–™é›†å»ºç«‹ CLIP ç‰¹å¾µç´¢å¼•

        Args:
            dataset_dir: è³‡æ–™é›†ç›®éŒ„ (å¦‚ 'dataset/')
            output_dir: è¼¸å‡ºç›®éŒ„
            batch_size: æ‰¹æ¬¡å¤§å°

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        dataset_dir = Path(dataset_dir)
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)

        logger.info(f"ğŸ”¨ é–‹å§‹å»ºç«‹è³‡æ–™é›†ç´¢å¼•: {dataset_dir}")

        # æ”¶é›†æ‰€æœ‰åœ–ç‰‡
        image_paths = []
        labels = []

        for class_dir in sorted(dataset_dir.iterdir()):
            if not class_dir.is_dir():
                continue

            class_name = class_dir.name
            class_images = list(class_dir.glob('*.png')) + list(class_dir.glob('*.jpg'))

            logger.info(f"ğŸ“ é¡åˆ¥: {class_name} - {len(class_images)} å¼µåœ–ç‰‡")

            for img_path in class_images:
                image_paths.append(img_path)
                labels.append(class_name)

        if len(image_paths) == 0:
            logger.error(f"âŒ åœ¨ {dataset_dir} ä¸­æ‰¾ä¸åˆ°åœ–ç‰‡")
            return False

        logger.info(f"ğŸ“Š ç¸½è¨ˆ: {len(image_paths)} å¼µåœ–ç‰‡, {len(set(labels))} å€‹é¡åˆ¥")

        # æ‰¹æ¬¡æå–ç‰¹å¾µ
        features, valid_paths = self.extract_batch_image_features(image_paths, batch_size)

        if features is None:
            return False

        # æ›´æ–°æ¨™ç±¤ï¼ˆåªä¿ç•™æˆåŠŸè™•ç†çš„ï¼‰
        valid_labels = []
        path_to_label = {str(path): label for path, label in zip(image_paths, labels)}
        for path in valid_paths:
            valid_labels.append(path_to_label[path])

        # å„²å­˜ç‰¹å¾µå’Œæ¨™ç±¤
        feature_file = output_dir / "clip_features.npy"
        label_file = output_dir / "clip_labels.pkl"
        path_file = output_dir / "clip_paths.pkl"

        np.save(feature_file, features)

        with open(label_file, 'wb') as f:
            pickle.dump(valid_labels, f)

        with open(path_file, 'wb') as f:
            pickle.dump(valid_paths, f)

        logger.info(f"ğŸ’¾ ç‰¹å¾µå·²å„²å­˜è‡³: {feature_file}")
        logger.info(f"ğŸ’¾ æ¨™ç±¤å·²å„²å­˜è‡³: {label_file}")
        logger.info(f"ğŸ’¾ è·¯å¾‘å·²å„²å­˜è‡³: {path_file}")
        logger.info(f"âœ… ç´¢å¼•å»ºç«‹å®Œæˆï¼")

        return True


def main():
    """ä¸»ç¨‹å¼ - å»ºç«‹ CLIP ç‰¹å¾µç´¢å¼•"""

    print("=" * 60)
    print("ğŸš€ CLIP Feature Extractor for STL Recognition")
    print("=" * 60)

    # åˆå§‹åŒ– CLIP æ¨¡å‹
    extractor = CLIPFeatureExtractor(model_name="ViT-B/32")

    # å»ºç«‹è³‡æ–™é›†ç´¢å¼•
    dataset_dir = Path("dataset")

    if not dataset_dir.exists():
        logger.error(f"âŒ è³‡æ–™é›†ç›®éŒ„ä¸å­˜åœ¨: {dataset_dir}")
        return

    success = extractor.build_dataset_index(
        dataset_dir=dataset_dir,
        output_dir=".",
        batch_size=32
    )

    if success:
        print("\n" + "=" * 60)
        print("âœ… CLIP ç‰¹å¾µç´¢å¼•å»ºç«‹æˆåŠŸï¼")
        print("=" * 60)
        print("\nç”Ÿæˆçš„æª”æ¡ˆï¼š")
        print("  ğŸ“„ clip_features.npy - CLIP ç‰¹å¾µå‘é‡")
        print("  ğŸ“„ clip_labels.pkl - é¡åˆ¥æ¨™ç±¤")
        print("  ğŸ“„ clip_paths.pkl - åœ–ç‰‡è·¯å¾‘")
        print("\nä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œ clip_faiss_search.py å»ºç«‹ FAISS ç´¢å¼•")
    else:
        print("\n" + "=" * 60)
        print("âŒ ç´¢å¼•å»ºç«‹å¤±æ•—")
        print("=" * 60)


if __name__ == "__main__":
    main()
