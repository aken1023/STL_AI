#!/usr/bin/env python3
"""
FAISS-based Image Recognition System
ä½¿ç”¨ FAISS ç›¸ä¼¼åº¦æœç´¢é€²è¡Œåœ–ç‰‡è­˜åˆ¥
"""
import os
import numpy as np
import cv2
import faiss
import pickle
from PIL import Image
import time
from sklearn.preprocessing import normalize
from torchvision import transforms, models
import torch
import torch.nn as nn

class FAISSRecognitionEngine:
    def __init__(self):
        self.index = None
        self.labels = None
        self.feature_extractor = None
        self.classes = []
        self.index_file = "faiss_features.index"
        self.labels_file = "faiss_labels.pkl"
        self.loaded = False

        # é è™•ç†è½‰æ›
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                               std=[0.229, 0.224, 0.225])
        ])

    def load_feature_extractor(self):
        """è¼‰å…¥ç‰¹å¾µæå–æ¨¡å‹"""
        import time
        current_time = time.strftime('%H:%M:%S')
        print(f"[{current_time}] ğŸ“¦ è¼‰å…¥ ResNet50 ç‰¹å¾µæå–å™¨...")

        # ä½¿ç”¨é è¨“ç·´çš„ ResNet50 (æ–°ç‰ˆ API)
        import warnings
        warnings.filterwarnings('ignore', category=UserWarning)

        try:
            # å˜—è©¦ä½¿ç”¨æ–°ç‰ˆ API (torchvision >= 0.13)
            from torchvision.models import ResNet50_Weights
            self.feature_extractor = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)
        except (ImportError, AttributeError):
            # é™ç´šåˆ°èˆŠç‰ˆ API
            self.feature_extractor = models.resnet50(pretrained=True)

        self.feature_extractor = nn.Sequential(*list(self.feature_extractor.children())[:-1])
        self.feature_extractor.eval()

        # ç§»å‹•åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        current_time = time.strftime('%H:%M:%S')
        if torch.cuda.is_available():
            self.feature_extractor = self.feature_extractor.cuda()
            print(f"[{current_time}] ğŸš€ ä½¿ç”¨ GPU åŠ é€Ÿ")
        else:
            print(f"[{current_time}] ğŸ’» ä½¿ç”¨ CPU é‹ç®—")

    def extract_features(self, image_path):
        """å¾åœ–ç‰‡æå–ç‰¹å¾µå‘é‡"""
        try:
            # è¼‰å…¥å’Œé è™•ç†åœ–ç‰‡
            image = Image.open(image_path).convert('RGB')
            input_tensor = self.transform(image).unsqueeze(0)

            if torch.cuda.is_available():
                input_tensor = input_tensor.cuda()

            with torch.no_grad():
                features = self.feature_extractor(input_tensor)
                features = features.flatten().cpu().numpy()

            # æ­£è¦åŒ–ç‰¹å¾µå‘é‡
            features = normalize([features])[0]
            return features

        except Exception as e:
            print(f"âŒ ç‰¹å¾µæå–å¤±æ•— {image_path}: {e}")
            return None

    def build_index(self, dataset_dir=None):
        """å»ºç«‹ FAISS ç´¢å¼•"""
        if dataset_dir is None:
            # ä½¿ç”¨æ¨™æº–è³‡æ–™é›†ç›®éŒ„
            dataset_dir = "dataset"

        if not os.path.exists(dataset_dir):
            print("âŒ æ‰¾ä¸åˆ°è³‡æ–™é›†ç›®éŒ„")
            return False

        print(f"ğŸ—ï¸  å»ºç«‹ FAISS ç´¢å¼•å¾ {dataset_dir}")

        # è¼‰å…¥ç‰¹å¾µæå–å™¨
        if self.feature_extractor is None:
            self.load_feature_extractor()

        features_list = []
        labels_list = []

        # æƒææ‰€æœ‰é¡åˆ¥
        classes = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]
        self.classes = sorted(classes)

        import time
        start_time = time.time()
        current_time = time.strftime('%H:%M:%S')

        print(f"[{current_time}] ğŸ“‚ æ‰¾åˆ° {len(self.classes)} å€‹é¡åˆ¥: {', '.join(self.classes[:5])}{'...' if len(self.classes) > 5 else ''}")

        # è¨ˆç®—ç¸½åœ–ç‰‡æ•¸
        total_images = 0
        for class_name in self.classes:
            class_dir = os.path.join(dataset_dir, class_name)
            images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
            total_images += len(images)

        current_time = time.strftime('%H:%M:%S')
        print(f"[{current_time}] ğŸ“Š ç¸½å…±éœ€è¦è™•ç† {total_images} å¼µåœ–ç‰‡ (ä¾†è‡ª {len(self.classes)} å€‹é¡åˆ¥)")
        print(f"[{current_time}] âš¡ å¹³å‡æ¯å€‹é¡åˆ¥: {total_images // len(self.classes)} å¼µåœ–ç‰‡")

        processed_count = 0

        for class_id, class_name in enumerate(self.classes):
            class_start_time = time.time()
            class_dir = os.path.join(dataset_dir, class_name)
            images = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]

            current_time = time.strftime('%H:%M:%S')
            print(f"[{current_time}] ğŸ” è™•ç†é¡åˆ¥ [{class_id+1}/{len(self.classes)}] {class_name}: {len(images)} å¼µåœ–ç‰‡")

            for img_idx, img_name in enumerate(images):
                img_path = os.path.join(class_dir, img_name)
                features = self.extract_features(img_path)

                if features is not None:
                    features_list.append(features)
                    labels_list.append({
                        'class_id': class_id,
                        'class_name': class_name,
                        'image_path': img_path
                    })

                processed_count += 1

                # æ¯è™•ç†10å¼µåœ–ç‰‡è¼¸å‡ºä¸€æ¬¡é€²åº¦
                if processed_count % 10 == 0 or processed_count == total_images:
                    progress_percent = (processed_count / total_images * 100)
                    elapsed = time.time() - start_time
                    if processed_count > 0:
                        avg_time_per_image = elapsed / processed_count
                        remaining_images = total_images - processed_count
                        estimated_remaining = avg_time_per_image * remaining_images
                        current_time = time.strftime('%H:%M:%S')
                        print(f"[{current_time}] â³ é€²åº¦: {processed_count}/{total_images} å¼µåœ–ç‰‡ ({progress_percent:.1f}%) | é ä¼°å‰©é¤˜: {estimated_remaining/60:.1f} åˆ†é˜")

            # æ¯å€‹é¡åˆ¥å®Œæˆå¾Œè¼¸å‡ºé€²åº¦
            class_elapsed = time.time() - class_start_time
            class_progress = ((class_id + 1) / len(self.classes) * 100)
            current_time = time.strftime('%H:%M:%S')
            print(f"[{current_time}] âœ… é¡åˆ¥ {class_name} è™•ç†å®Œæˆ ({len(images)} å¼µ, {class_elapsed:.1f}ç§’) | ç¸½é€²åº¦: {class_progress:.1f}%")

        if len(features_list) == 0:
            current_time = time.strftime('%H:%M:%S')
            print(f"[{current_time}] âŒ æ²’æœ‰æˆåŠŸæå–ä»»ä½•ç‰¹å¾µ")
            return False

        # è½‰æ›ç‚º numpy é™£åˆ—
        current_time = time.strftime('%H:%M:%S')
        print(f"[{current_time}] ğŸ”„ è½‰æ›ç‰¹å¾µç‚º numpy é™£åˆ—...")
        features_array = np.array(features_list, dtype=np.float32)

        current_time = time.strftime('%H:%M:%S')
        print(f"[{current_time}] ğŸ“Š ç‰¹å¾µç¶­åº¦: {features_array.shape}")
        print(f"[{current_time}] ğŸ“Š ç‰¹å¾µå‘é‡æ•¸: {features_array.shape[0]} å€‹")
        print(f"[{current_time}] ğŸ“Š ç‰¹å¾µç¶­åº¦å¤§å°: {features_array.shape[1]} ç¶­")

        # å»ºç«‹ FAISS ç´¢å¼•
        current_time = time.strftime('%H:%M:%S')
        print(f"[{current_time}] ğŸ—ï¸  å»ºç«‹ FAISS ç´¢å¼• (IndexFlatIP)...")
        dimension = features_array.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å…§ç©ç›¸ä¼¼åº¦
        self.index.add(features_array)
        self.labels = labels_list

        # å„²å­˜ç´¢å¼•
        current_time = time.strftime('%H:%M:%S')
        print(f"[{current_time}] ğŸ’¾ å„²å­˜ FAISS ç´¢å¼•...")
        self.save_index()

        total_elapsed = time.time() - start_time
        current_time = time.strftime('%H:%M:%S')
        print(f"[{current_time}] âœ… FAISS ç´¢å¼•å»ºç«‹å®Œæˆï¼")
        print(f"[{current_time}] ğŸ“Š ç‰¹å¾µå‘é‡ç¸½æ•¸: {self.index.ntotal} å€‹")
        print(f"[{current_time}] ğŸ“‚ é¡åˆ¥ç¸½æ•¸: {len(self.classes)} å€‹")
        print(f"[{current_time}] â±ï¸  ç¸½è€—æ™‚: {total_elapsed:.1f} ç§’ ({total_elapsed/60:.1f} åˆ†é˜)")
        print(f"[{current_time}] âš¡ å¹³å‡è™•ç†é€Ÿåº¦: {processed_count/total_elapsed:.1f} å¼µåœ–ç‰‡/ç§’")
        return True

    def save_index(self):
        """å„²å­˜ FAISS ç´¢å¼•å’Œæ¨™ç±¤"""
        try:
            faiss.write_index(self.index, self.index_file)
            with open(self.labels_file, 'wb') as f:
                pickle.dump({
                    'labels': self.labels,
                    'classes': self.classes
                }, f)
            print(f"ğŸ’¾ ç´¢å¼•å·²å„²å­˜è‡³ {self.index_file} å’Œ {self.labels_file}")
        except Exception as e:
            print(f"âŒ å„²å­˜ç´¢å¼•å¤±æ•—: {e}")

    def load_index(self):
        """è¼‰å…¥ FAISS ç´¢å¼•å’Œæ¨™ç±¤"""
        try:
            if not os.path.exists(self.index_file) or not os.path.exists(self.labels_file):
                print("âš ï¸  ç´¢å¼•æª”æ¡ˆä¸å­˜åœ¨ï¼Œéœ€è¦å…ˆå»ºç«‹ç´¢å¼•")
                return False

            self.index = faiss.read_index(self.index_file)
            with open(self.labels_file, 'rb') as f:
                data = pickle.load(f)
                self.labels = data['labels']
                self.classes = data['classes']

            # è¼‰å…¥ç‰¹å¾µæå–å™¨
            if self.feature_extractor is None:
                self.load_feature_extractor()

            self.loaded = True
            print(f"âœ… FAISS ç´¢å¼•è¼‰å…¥æˆåŠŸï¼ŒåŒ…å« {self.index.ntotal} å€‹ç‰¹å¾µå‘é‡")
            print(f"ğŸ“‹ é¡åˆ¥: {', '.join(self.classes)}")
            return True

        except Exception as e:
            print(f"âŒ è¼‰å…¥ç´¢å¼•å¤±æ•—: {e}")
            return False

    def predict(self, image_path, k=5):
        """ä½¿ç”¨ FAISS é€²è¡Œåœ–ç‰‡è­˜åˆ¥"""
        if not self.loaded:
            print("âŒ FAISS ç´¢å¼•æœªè¼‰å…¥")
            return None

        start_time = time.time()

        # æå–æŸ¥è©¢åœ–ç‰‡çš„ç‰¹å¾µ
        query_features = self.extract_features(image_path)
        if query_features is None:
            return None

        # æœç´¢æœ€ç›¸ä¼¼çš„ç‰¹å¾µ
        query_features = query_features.reshape(1, -1).astype(np.float32)
        similarities, indices = self.index.search(query_features, k)

        inference_time = (time.time() - start_time) * 1000

        # æ•´ç†çµæœ
        predictions = []
        for i, (similarity, idx) in enumerate(zip(similarities[0], indices[0])):
            if idx < len(self.labels):
                label = self.labels[idx]
                predictions.append({
                    'class_id': label['class_id'],
                    'class_name': label['class_name'],
                    'confidence': float(similarity),
                    'rank': i + 1,
                    'reference_image': label['image_path']
                })

        # æŒ‰é¡åˆ¥çµ±è¨ˆæŠ•ç¥¨
        class_votes = {}
        for pred in predictions:
            class_name = pred['class_name']
            if class_name not in class_votes:
                class_votes[class_name] = {'count': 0, 'total_confidence': 0}
            class_votes[class_name]['count'] += 1
            class_votes[class_name]['total_confidence'] += pred['confidence']

        # è¨ˆç®—æœ€çµ‚çµæœ
        final_predictions = []
        for class_name, vote_data in class_votes.items():
            avg_confidence = vote_data['total_confidence'] / vote_data['count']
            final_predictions.append({
                'class_id': self.classes.index(class_name),
                'class_name': class_name,
                'confidence': avg_confidence,
                'vote_count': vote_data['count']
            })

        # æŒ‰ç½®ä¿¡åº¦æ’åº
        final_predictions.sort(key=lambda x: x['confidence'], reverse=True)

        return {
            'predictions': final_predictions,
            'detailed_results': predictions,
            'inference_time': inference_time,
            'method': 'FAISS',
            'success': True
        }

# å…¨åŸŸ FAISS å¼•æ“å¯¦ä¾‹
faiss_engine = FAISSRecognitionEngine()

def initialize_faiss():
    """åˆå§‹åŒ– FAISS å¼•æ“"""
    print("ğŸš€ åˆå§‹åŒ– FAISS è­˜åˆ¥å¼•æ“")

    # å˜—è©¦è¼‰å…¥ç¾æœ‰ç´¢å¼•
    if not faiss_engine.load_index():
        print("ğŸ“š ç´¢å¼•ä¸å­˜åœ¨ï¼Œé–‹å§‹å»ºç«‹æ–°ç´¢å¼•...")
        if faiss_engine.build_index():
            print("âœ… FAISS åˆå§‹åŒ–æˆåŠŸ")
            return True
        else:
            print("âŒ FAISS åˆå§‹åŒ–å¤±æ•—")
            return False
    else:
        print("âœ… FAISS åˆå§‹åŒ–æˆåŠŸ")
        return True

def predict_with_faiss(image_path):
    """ä½¿ç”¨ FAISS é€²è¡Œé æ¸¬"""
    if not faiss_engine.loaded:
        print("âš ï¸  FAISS æœªåˆå§‹åŒ–ï¼Œå˜—è©¦åˆå§‹åŒ–...")
        if not initialize_faiss():
            return None

    return faiss_engine.predict(image_path)

if __name__ == "__main__":
    # æ¸¬è©¦ FAISS å¼•æ“
    print("ğŸ§ª æ¸¬è©¦ FAISS è­˜åˆ¥å¼•æ“")

    if initialize_faiss():
        # æ¸¬è©¦é æ¸¬
        test_images = []
        for root, dirs, files in os.walk("dataset"):
            for file in files[:1]:  # æ¯å€‹ç›®éŒ„æ¸¬è©¦ä¸€å¼µåœ–ç‰‡
                if file.lower().endswith(('.png', '.jpg')):
                    test_images.append(os.path.join(root, file))

        for img_path in test_images[:3]:  # æ¸¬è©¦å‰3å¼µ
            print(f"\nğŸ” æ¸¬è©¦åœ–ç‰‡: {img_path}")
            result = predict_with_faiss(img_path)
            if result:
                print(f"â±ï¸  æ¨è«–æ™‚é–“: {result['inference_time']:.1f}ms")
                for pred in result['predictions'][:3]:
                    print(f"  ğŸ“Š {pred['class_name']}: {pred['confidence']:.3f} (æŠ•ç¥¨: {pred['vote_count']})")
    else:
        print("âŒ FAISS å¼•æ“æ¸¬è©¦å¤±æ•—")